카프카 정리

//https://kafka.apache.org/documentation/streams/developer-guide/processor-api.html

Zookeeper와 kafaka의 관계
kafka는 내부적으로 Zookeeper를 사용하여 클러스터의 메타데이터 관리, 리더 선출, 구성관리, 그리고 브로커의 살아있음 확인(health checking)과 같은
여러 중요한 작업을 수행
 - 클러스터 메타데이터 관리 : kafka 클러스터의 모든 브로커는 Zookeeper에 등록되어 있으며, Zookeeper는 이러한 브로커들의 상태와 메타데이터 정보를
                                         관리합니다.
 - 리더 선출 : kafka의 파티션에는 리더와 팔로워가 있으며, 모든 읽기 및 쓰기 요청은 리더를 통해 처리. Zookeeper는 이 리더를 동적으로 선출하고 관리함
 - 구성 관리 : kafak 클러스터의 구성 변경사항이 있을 때, 이 변경사항을 Zookeeper에 저장하고 모든 브로커가 일관된 구성을 사용하도록 함
 - 브로커의 살아있음 확인 : Zookeeper는 kafka 브로커가 살아있는지 확인하기 위한 메커니즘을 제공, 브로커가 죽었을 때 다른 브로커에게 알려주어
                                     리밸런싱을 수행 함 
이러한 방식으로 , Zookeeper는 kafka 클러스터의 안정적이고 일관된 운영을 지원하는 역할을 함.

토픽 개념 
마치 rdbs에 테이블 이라는 개념 어떤 특징을 나타내는 데이터처럼 구분하는 특징 토픽 만들어수 구성
파티션 하나이상 운영가능 

프로듀서 
토픽에 파티션으로 데이터로 보내는 역할 하나의 파티션으로 보낸다. 
프로듀서가 메시지를 보낸다(데이터를 보낸다) 

파티션
파티션 내부 구조는 Queue 구조 FIFO방식의 큐자료와 유사함 
큐에 데이터를 보내는 것이 '프로듀서' 이고 큐에서 데이터를 가져가는 것이 '컨슈머' 이다 
컨슈머가 파티션에 데이터를 가져가더라도 삭제되지 않는다.

컨슈머 
여러 파티션을 구독해서 메시지를 가져간다. 
적재된 데이터를 가져간다 
내부적으로 '커밋' 이라고 어디까지 파티션에 데이터를 읽었는지 기록하고 있다. 


카프카가 데이터 파이프라인으로 적합한 4가지 이유 
1. 높은 처리량
많은 양의 데이터를 송수신할 때 맺어지는 네트워크 비용은 무시할 수 없다.
카프카는 많은 양의 데이터를 묶음 단위로 처리하는 배치로 빠르게 처리할 수 있기 때문에 대용량의 실시간 로그데이터를 처리하는 데에 적합 하다. 
파티션 개수만큼 컨슈머 개수를 늘려서 동일 시간당 데이터 처리 량을 늘릴수 있다.
스케일아웃(인스턴스개수)를 늘려서 전체 처리량을 늘릴수있다 스케일업 아니다

2. 확장성 
데이터 파이프라인에서 데이터를 모을 떄 데이터가 얼마나 들어올지 예측이 힘듬. 
특정 이벤트로 인해 100만 건 이상의 데이터가 들어오는 경우? 
데이터가 적을때 브로커를 최소한의 개수로 운영하고 데이터가 많아지면 클러스터의 브로커 개수를 자연스럽게 늘려 스케일 아웃 할 수 있다. 
스케일아웃, 스케일 인 과정은 클러스터의 무중단 운영을 지원하므로 365일 24시간 데이터를 처리해야 하는 커머스나 은행 같은 비즈니스 모델에서도 안정적인 운영이 가능하다. 

3. 영속성
영속성이란 데이터를 생성한 프로그램이 종료되더라도 사라지지 않은 데이터의 특성을 뜻한다. 
카프카는 다른 메시징 플랫폼과 다르게 전송받은 데이터를 메모리에 저장하지 않고 파일 시스템에 저장한다. 운영체제에서 파일시스템을 활용 페이지 캐시 영역을 메모리에 따라 생성하여 사용
장애 발생시 프로세스를 재시작하여 안전하게 데이터 처리 

4. 고가용성 
3개 이상의 서버들로 운영되는 카프카 클러스터는 일부 서버에 장애가 발생하더라도 무중단으로 안전하고 지속적으로 데이터를 처리할 수 있다.

빅데이터 아케텍처의 종류 카프카의 미래 
람다 아키텍처 (배치 레이어, 서빙레이어, 스피드 레이어)
스피드 레이어에 카프카가 속해있다 .

카파 아키텍처 (스피드 레이터, 서빙 레이어) 
카프카를 최초로 고안 한 개발자 제이 클렙스 ,람다 아키텍처의 단점해소하기 위해 카프카 개발 

스트리밍 데이터 레이크 (스피드 레이어)


퀴즈
1) 카프카는 메모리에 데이터를 저장하기 때문에 장애가 났을 경우 데이터가 유실 된다. 
아니다. 프로듀서에서 데이터를 보내게 되면 카프카 브로커 프로세스에 데이터가 들어오게되면 내부적으로 파일시스템에 데이터를 .log로 데이터가 저장한다. 

2) 카파 아키텍처는 람다 아케텍처를 개선한 데이터 아키텍처이다 
맞다 
배치레이어, 스피드레이어, 서빙레이어 람다아키텍처를 개선한게 카파 아키텍처이다.

3) 서빙 레이어는 데이터를 프로세싱하는데 사용되는 곳이다. 
아니다. 서빙 레이어는 사용자가 데이터를 가져가도록 하거나 클라이언트에 활용되도록 하는 역할을 한다.


카프카 생태계 

프로튜서 -------------------->카프카 클러스터     
                                          토픽 0              ->  커넥트 (싱크)   -> JDBC      
 MySQL  -> 커넥트                토픽 1              ->  커넥트 (싱크)   -> Elasticsearch      
AWS S3  -> (소스)                 토픽 2              -----------------------> 컨슈머
                                                                       |
                                          토픽 3                 <-| 스트림즈

토픽2에 있는걸 토픽3에 스테이트풀 또는 스테이플리스 하게 넣고싶다면 카프카 스트림즈 라이브러리 사용 
프로듀서, 커넥트(소스), 커넥트(싱크), 스트림즈, 컨슈머 는 오픈소스의 자바 기능이다. 

커넥트는 데이터파이프라인을 운영하는 가장 핵심적인 tool 이다 
커넥트(소스)는 프로듀서 역할, 커넥트(싱크)는 컨슈머의 역할 타겟어플리케이션으로 데이터를 보내는 역할 


카프카 브로커와 클러스터 
주키퍼 - 카프카 클러스터를 운영하기 위해 반드시 필요한 어플리케이션 카프카2. 대 까지 필요 3. 주키퍼가 없어도 됨 

카프카 클러스터
한개의 카프카 클러스터는 여러개의 브로커가 존재(브로커는 하나의 물리적인 환경 인스턴스) 
데이터를 분산 저장하여 장애가 발생하더라도 안전하게 사용할 수 있도록 도와주는 애플리케이션이다.
브로커 하나하나가 프로세스라 봐도 된다. 안전하게 운용하기 위해서 3개이상의 브로커를 1개의 카프카 클러스터로 운영 
카프카 클러스터로 묶인 브로커들은 프로듀서가 보낸 데이터를 안전하게 분산 저장하고 복제하는 역할을 수행한다.

여러개의 카프카 클러스터가 연결된 주키퍼 
- 카프카 클러스터를 실행하기 위해서는 주키퍼가 필요함
- 주키퍼의 서로 다른 znode에 클러스터를 지정하면 됨 
- root znode에 각 클러스터별 znode를 생성하고 클러스터 실행시 root가 아닌 하위 znode로 설정 
- 카프카 3.0 부터는 주키퍼가 없어도 클러스터 동작 가능 

카프카 브로커의 역할 
 - 컨트롤러 역할 
클러스터의 다수 브로커 중 한대가 컨트롤러의 역할을 한다. 컨트롤러는 다른 브로커들의 상태를 체크하고 브로커가 클러스터에서 빠지는 경우
해당 브로커에 존재하는 리더 파티션을 재분배한다. 카프카는 지속적으로 데이터를 처리해야 하므로 브로커의 상태가 비정상이라면 빠르게 클러스터에서
빼내는 것이 중요하다. 만약 컨트롤러 역할을 하는 브로커에 장애가 생기 면 다른 브로커가 컨트롤러 역할을 한다.

 - 데이터 삭제 역할
카프카는 다른 메시징 플랫폼과 다르게 컨슈머가 데이터를 가져가더라도 토픽의 데이터는 삭제되지 않는다. 
또한, 컨슈머나 프로듀서가 데이터 삭제를 요청할 수도 없다. 오직 브로커만이 데이터를 삭제할 수 있다.
데이터 삭제는 파일 단위로 이루어지는데 이 단위를 '로그 세그먼트(log segment)'라고 부른다. 
이 세그먼트에는 다수의 데이터가 들어 있기 때문에 일반적인 데이터베이스처럼 특정 데이터를 선벽해서 삭제 할 수 없다.

- 컨슈머 오프셋 저장 
컨슈머 그룹은 토픽이 특정 파티션으로부터 데이터를 가져가서 처리하고 이 파티션의 어느 레코드까지 가져갔는지 확인하기 위해 오프셋을 커밋한다.
커밋한 오프셋은 __consumer__offsets 토픽에 저장한다. 여기에 저장된 오프셋을 토대로 컨슈머 그룹은 다음 레코드를 가져가서 처리한다. 

- 그룹 코디네이터 
코데네이터는 컨슈머 그룹의 상태를 체크하고 파티션을 컨슈머와 매칭되도록 분배하는 역할을 한다.
컨슈머가 컨슈머 그룹에서 빠지면 매칭되지 않은 파티션을 정상 동작하는 컨슈머로 할당하여 끊임없이 데이터가 처리되도록 도와준다.
이렇게 파티션을 컨슈머로 재할당하는 과정을 '리밸런스(rebalance)'라고 부른다.

브로커 로그와 세그먼트 
브로커의 역할 - 데이터의 저장 
 - 카프카를 실핼할때 config/server.properties의 log.dir 옵션에 정의한 디렉토리에 데이터를 저장한다.
토픽 이름과 파티션 번호의 조합으로 하위 디렉토리를 생성하여 데이터를 저장한다. 
- hello.kafka 토픽의 0번 파티션에 존재하는 데이터를 확인할 수 있는대. log에는 메시지와 메타데이터를 저장한다.
index는 메시지의 오프셋을 인덱싱한 정보를 담은 파일이다. timeindex 파일에는 메시지에 포함된 timestamp값을 기준으로 인덱싱한 정보가 담겨있다.
메시지는 레코드라고 부르고 프로듀서가 만들어낸것을 뜻함 메시지키 : 값 포함 

로그와 세그먼트 개념을 알아야 한다. 
데이터가 한파일에 연속적으로 저장되는게 아니라 레코드별 파일로 나누어짐 
 - log.segment.bytes : 바이트 단위의 최대 세그먼트 크기 지정. 기본 값은 1GB.
 - log.roll.ms(hours) : 세그먼트가 신규 생성된 이후 다음 파일로 넘어가는 시간 주기, 기본 값은 7일.
 - 액티브 세그먼트
    가장 마지막 세그먼트 파일(쓰기가 일어나고 있는 파일)을 액티브 세그먼트라고 부른다. 
    액티브 세그먼트는 브루커의 삭제 대상에서 포함되지 않는다. 액티브 세그먼트가 아닌 세그먼트는 retention 옵션에 따라 삭제 대상으로 지정된다.

세그먼트와 삭제 주기 ( cleanup.policy = delete)
 - 액티브가 아닌 로그파일 삭제할 수 있다. 
 - retuention.ms(minute, hours) : 세그먼트를 보유할 최대 기간. 기본 값은 7일. 일반적으로 3일도 자주 사용
 - retention.bytes : 파티션당 로그 적재 바이트 값. 기본 값은 -1(지정하지 않음) 1기가 
 - log.retention.check.interval.ms : 세그먼트가 삭제 영역에 들어왔는지 확인하는 간격. 기본 값은 5분.
 - 카프카에서 데이터는 세그먼트 단위로 삭제가 발생하기 때문에 로그 단위(레코드 단위)로 개별 삭제는 불가능 하다.
   또한, 로그(레코드)의 메시지 키, 메시지 값, 오프셋, 헤더 등 이미 적재된 데이터에 대해서 수정 또한 불가능하기 때문에 
   데이터를 적재할 때(프로듀서) 또는 데이터를 사용할 때(컨슈머) 데이터를 검증하는 것이 좋다.

 cleanup.policy = compact 
 - 토픽 압축 정책은 일반적으로 생각하는 zip과 같은 압축(compression)과는 다른 개념.
   여기서 압축이란 메시지 키 별로 해당 메시지 키의 레코드 중 오래된 데이터를 삭제하는 정책을 뜻한다. 그렇기 때문에 삭제(delete) 정책과 다르게
   일부 레코드만 삭제가 될 수 있다. 압축은 액티브 세그먼트를 제외한 데이터가 대상이다.
   중복되는 키중 가장 최신의 메시지 키만 남기고 삭제 

 테일/헤드 영역, 클린/더티 로그
 - 테일 영역 : 압축 정책에 의해 압축이 완료된 레코드들, 클린(clean)로그 라고도 부른다. 중복 메시지 키가 없다
 - 헤드 영역 : 압축 정책이 되기 전 레코드들. 더티(dirty)로그 라고도 부른다. 중복된 메시지 키가 있다.

 min.cleanable.dirty.ratio
 - 데이터의 압축 시작 시점은 min.cleanable.dirty.ratio 옵션값을 따른다. min.cleanable.dirty.ratio 옵션 값은 액티브 세그먼트를 제외한
   세그먼트에 남아 있는 테일 영역의 레코드 개수와 헤드 영역의 레코드 개수의 비율을 뜻한다.
   예를 들어 0.5로 설정한다면 테일 영역의 레코드 개수가 헤드 영역의 레코드 개수와 동일할 경우 압축이 실행된다.
   0.9와 같이 크게 설정하면 한번 압축을 할 때 많은 데이터가 줄어들므로 압축 효과가 좋다. 그러나 0.9 비율이 될 때까지 용량을 차지하므로 용량 효율이 좋지 않다.
   반면 0.1과 같이 작게 설정하면 압축이 자주 일어나서 가장 최신 데이터만 유지할 수 있지만 압축이 자주 발생하기 때문에 브로커에 부담을 줄 수 있다.

브로커의 역할 - 복제(replication)
 - 데이터 복제(replication)는 카프카를 장애 허용 시스템(fault tolerrant system)으로 동작하도록 하는 원동력이다.
   복제의 이유는 클러스터로 묶인 브로커 중 일부에 장애가 발생하더라도 데이터를 유실하지 않고 안전하게 사용하기 위함이다.
   카프카의 데이터 복제는 파티션 단위로 이루어진다.
   토픽을 생성할 때 파티션의 복제 개수(replication factor)도 같이 설정되는데 직접 옵션을 선택하지 않으면 브로커에 설정된 옵션 값을 따라간다.
   복제 개수의 최솟값은 1(복제없음)이고 최대값은 브로커 개수만큼 설정하여 사용할 수 있다.

   복제된 파티션은 리더(leader)와 팔로워(follower)로 구성된다. 프로듀서 또는 컨슈머와 직접 통신하는 파티션을 리더, 나머지 복제 데이터를 
   가지고 있는 파티션을 팔로워라고 부른다.
   통상적으로 리더와 팔로워라고 부르지만 이 책에서는 파티션임을 명확히 하기 위해 파티션의 리더를 '리더 파티션', 파티션의 팔로워를
   '팔로워 파티션'이라고 지칭한다. 팔로워 파티션들은 리더 파티션의 오프셋을 확인하여 현재 자신이 가지고 있는 오프셋과 차이가 나는 경우 
   리더 파이션으로 부터 데이터를 가져와서 자신의 파티션에 저장하는데, 이 과정을 '복제'라고 부른다.

   파티션 복제로 인해 나머지 브로커에도 파티션의 데이터가 복제되므로 복제 개수만큼의 저장 용량이 증가 하는 단점이 존재한다.
   그러나 복제를 통해 데이터를 안전하게 사용할수 있다는 강력한 장점때문에 카프카를 운영할 때 2개 이상의 복제 개수를 정하는 것이 중요하다.
   카프카 브로커가 설퇴된 기업용 서버는 개인용 컴퓨터와 비교가 안될정도로 안정성이 좋지만 서버는 해커로 인한 침입, 디스크 오류, 네트워크 연결장애
   등의 이유로 언제든지 장애가 발생할 수 있다.

   복제되었을때 repicication factor에  따라서 브로커만큼 용량이 곱하기 된다. 대신에 장애 대응이 가능 (큰 장점)

   브로커가 다운되면 해당 브로커에 있는 리더파티션은 사용할 수 없기 때문에 팔로워파티션 중 하나가 리더 파티션 지위를 넘겨받는다.
   이를 통해 데이터가 유실되지 않고 컨슈머나 프로듀서와 데이터를 주고받도록 동작 할 수 있다.
   운영 시에는 데이터 종류마다 다른 복제 개수를 설정하고 상황에 따라서는 토픽마다 복제 개수를 다르게 설정하여 운영하기도 한다.
   데이터가 일부 유실되어도 무관하고 데이터 처리 속도가 중요하다면 1 또는 2로 설정한다.
   금융 정보와 같이 유실이 일어나면 안되는 데이터의 경우 복제개수를 3으로 설정하기도 한다.
   리더 -> 팔로워로 리더로 바뀌는게 '승급' 이라 한다.
 
ISR(In-Sync-Replicas)
 - ISR은 리더 파티션과 팔로워 파티션이 모두 싱크가 된 상태를 뜻 한다. (오프셋의 개수가 같다)
   복제 개수가 2인 토픽을 가정해 보자. 이 토픽에는 리더 파티션 1개와 팔로워 파티션이 1개가 존재한다. 
   리더 파티션에 0부터 3의 오프셋이 있다고 가정할 때, 팔로워 파티션에 동기화가 관료되려면 0부터 3까지 오프셋이 존재해야 한다.
   동기화가 완료됐다는 의미는 리더 파티션의 모든 데이터가 팔로워 파티션에 복제된 상태를 말한다.

 - unclean.leader.election.enable
  리더 파티션의 데이터를 모두 복제하지 못한 상태이고, 이렇게 싱크가 되지 않은 팔로워 파티션이 리더 파티션으로 선출되면 데이터가 유실될 수 있다.
  유실이 발생하더라도 서비스를 중단하지 않고 지속적으로 토픽을 사용하고 싶다면 ISR이 아닌 팔로워 파티션을 리더로 선출하도록 설정할 수 있다.
  - unclean.leader.election.enable = true : 유실을 감수함. 복제가 안된 팔로워 파티션을 리더로 승급.
  - unclean.leader.election.enable = false : 유실을 감수하지 않음. 해당 브로커가 복구될 때까지 중단.

토픽과 파티션 
 - 토픽은 카프카에서 데이터를 구분하기 위해 사용하는 단위이다. 토픽은 1개 이상의 파티션을 소유하고 있따.
   파티션은 자료구조에서 접하는 큐(queue)와 비슷한 구조라고 생각하면 쉽다. First-in-first-out(FIFO) 구조와 같이 먼저 들어간 레코드는
   컨슈머가 먼저 가져가게 된다. 다만, 일반적인 자료구조로 사용되는 큐는 데이터를 가져가면(pop) 삭제하지만 카프카에서는 삭제 하지 않느다.
   파티션의 레코드는 컨슈머가 가져가는 것과 별개로 관리된다.
   이러한 특징 때문에 토픽의 레코드는 다양한 목적을 가진 여러 컨슈머 그룹들이 토픽의 데이터를 여러번 가져갈 수 있다.

 - 토픽 생성시 파티션이 배치되는 방법
   파티션이 5개인 토픽을 생성했을 경우 0번 브로커부터 시작하여 round-robin 방식으로 리더 파티션들이 생성된다.
   카프카 클라이언트는 리더 파티션이 있는 브로커와 통신하여 데이터를 주고 받으므로 여러 브로커에 골고루 네트워크 통신을 하게 된다. 이를 통해, 
   데이터가 특정 서버(브로커)와 통신이 집중되는(hot spot) 현상을 막고 선형 확장(linear scale out)을 하여 데이터가 많아지더라도 자연스럽게 대응할 수
   있다.
   프로듀서가 토픽 생성시 한 브로커에 치우치지 않고 각각 브로커에 토픽을 생성해서 통신이 집중되는 현상을 막는다.

   가끔마다 특정 브로커에 파티션이 네트워크가 쏠리는 현상이 생긴다.
   카프카 클러스터를 운영할때 파티션 몰리는것을 막아야 한다 
   kafka-reassign-partitions.sh  명령으로 파티션을 재분배 할 수 있다. 

파티션 개수와 컨슈머 개수의 처리량 
 - 파티션은 카프카의 병렬처리의 핵심으로써 그룹으로 묶인 컨슈머들이 레코드를 병렬로 처리 할 수 있도록 매칭된다. 
   컨슈머의 처리량이 한정된 상황에서 많은 레코드를 병렬로 처리하는 가장 좋은 방법은 컨슈머의 개수를 늘려 스케일 아웃하는 것이다.
   컨슈머 개수를 늘림과 동시에 파티션 개수도 늘리면 처리량이 증가하는 효과를 볼 수 있다.   
   프로듀서가 초당 10개의 데이터를 보내고 있고 컨슈머는 초당 1개를 처리한다고 했을때 컨슈머렉이 생김 
   보통 위의 케이스라면 파티션을 20개 컨슈머 20개를 잡으면 데이터 지연이 발생하지 않는다.

파티션 개수를 줄이는 것은 불가능
 - 카프카에서 파티션 개수를 줄이는 것은 지원하지 않는다. 카프카의 특징.. 그러므로 파티션을 늘리는 작업을 할 때는 신중히 파티션 개수를 정해야 한다.
   한번 늘리면 줄이는 것은 불가능하기 때문에 토픽을 삭제하고 재생성하는 방법 외에는 없기 때문이다.
   카프카에서는 파티션의 데이터를 세그먼트로 저장하고 있으며 만에 하나 지원을 한다고 하더라도 여러 브로커에 저장된 데이터를 취합하고 정렬해야하는
   복잡한 과정을 거쳐야 하기 때문에 클러스터에 큰 영향이 가게 된다. KIP-694에서 파티션 개수를 줄이는 것을 논의 했지만 더 이상 진행 되지 않고 있다.

레코드
 - 레코드는 타임스탬프, 헤더, 메시지 키, 메시지 값, 오프셋으로 구성되어 있다. 프로듀서가 생성한 레코드가 브로커로 전송되면 오프셋과 티임스탬프가 지정되어 저장된다. 레코드가 브로커에 전송되면 오프셋 생성되고 브로커에 한번 적재된 레코드는 수정할 수 없고 로그 리텐션 기간 또는 용량에 따라서만 
삭제된다.

레코드-타임스탬프
 - 레코드의 타임스탬프는 스트림 프로세싱에서 활용하기 위한 시간을 저장하는 용도로 사용된다. 
   카프카 0.10.0.0 이후 버전부터 추가된 타임스탬프는 Unix timestamp가 포함되며 프로듀서에서 따로 설정하지 않으면 기본값으로 ProducerRecord 생성
   시간(CreateTime)이 들어간다. 또는 브로커 적재 시간(LogAppendTime)으로 설정할 수도 있다. 해당 옵션은 토픽 단위로 설정가능하다.
   message.timestamp.type 을 사용한다.

레코드-오프셋
 - 레코드의 오프셋은 프로듀서가 생성한 레코드에는 존재하지 않는다. 프로듀서가 전송한 레코드가 브로커에 적재될 때 오프셋이 지정된다.
   오프셋은 0부터 시작되고 1씩 증가한다. 컨슈머는 오프셋을 기반으로 처리가 완료된 데이터와 앞으로 처리해야할 데이터를 구분한다.
   각 메시지는 파티션별로 고유한 오프셋을 가지므로 컨슈머에서 중복 처리를 방지하기 위한 목적으로도 사용한다.

레코드-헤더
 - 레코드의 헤더는 0.11부터 제공된 기능이다. key/value 데이터를 추가할 수 있으며 레코드의 스키마 버전이나 포맷과 같이 데이터 프로세싱에 참고할만한
   정보를 담아서 사용할 수 있다.

레코드-key
 - 메세지 키는 처리하고 자흔 메시지 값의 분류하기 위한 용도로 사용되며, 이를 파티셔닝이라고 부른다.
   파티셔닝에 사용하는 메시지 키는 파티셔너(Patitioner)에 따라 토픽의 파티션 번호가 정해진다.
   메시지 키가 null인 레코드는 특정 토픽의 파티션에 라운드 로빈으로 전달된다.
   null이 아닌 메시지 키는 해쉬값에 의해서 특정 파티션에 매핑되어 전달된다.(기본 파티셔너의 경우)

레코드-value
 - 레코드의 메시지 값은 실질적으로 처리할 데이터가 담기는 공간이다. 메시지 값의 포맷은 제네릭으로 사용자에 의해 지정된다.
   Float, Byte[], String 등 다양한 형태로 지정 가능하며 필요에 따라 사용자 지정 포맷으로 직렬화/역질렬화 클래스를 만들어 사용할 수도 있다.
   브로커에 저장된 레코드의 메시지 값은 어떤 포맷으로 직렬화되어 저장되었는지 알 수 없기 때문에 컨슈머는 미리 역질렬화 포맷을 알고 있어야 한다.

토픽 이름 제약 조건
 - 빈 문자열 토픽 이름은 지원하지 않는다.
 - 토픽 이름은 마침표 하나 또는 마침표 둘로 생성될 수 없다.
 - 토픽 이름의 길이는 249자 미만으로 생성되어야 한다.
 - 토픽 이름은 영어 대, 소문자와 숫자 0부터 9 그리고 마친표, 언더바, 하이픈 조합으로 생성할 수 있다.이외의 문자열이 포함되면 토픽 생성 불가
 - 카프카 내부 로직 관리 목적으로 사용되는 2개토픽(__consumer_offset, __tansaction_state)과 동일한 이름으로 생성 불가능하다.
 - 카프카 내부적으로 사용하는 로직 때문에 토픽 이름에 마침표와 언더바가 동시에 들어가면 안된다.
   생성은 할 수 있지만 사용시 이슈가 발생할 수 있기 때문에 마침표와 언더바가 들어간 토픽 이름을사용하면 WARNING 메시지가 발생한다.

의미 있는 토픽 이름 작명 방법
 - 토픽 이름에 대한 규칙을 사전에 정의하고 구성원들이 그 규칙을 잘 따르는 것
 - 카프카는 토픽 이름 변경을 지원하지 않는다. 삭제 후 다시생성해야 한다. 

토픽 작명의 템플릿과 예시 
<환경>.<팀-명>.<애플리케이션-명>.<메시지-타입>
prd.rnd-team.notification.json
<프로젝트-명>.<서비스-명>.<환경>.<이벤트-명>
doodoo.payment.dev.notification


클라이언트 메타데이터 
                          메타데이터 요청
                              ------------->    
카프카 클라이언트    <------------        카프카 클러스터(브로커)
 (프로듀서,컨슈머)   메타데이터 응답

- 카프카 클라이언트는 통신하고자 하는 리더 파티션의 위치를 알기 위해 데이터를 주고(프로듀서) 받기(컨슈머) 전에 메타데이터를 브로커로부터 
   전달받는다. 메타데이터는 다음과 같은 옵션을 통해 리프래쉬된다.
   리더 파티션이 어디에 있는지 메타데이터로 확인.
   카프카 프로듀서 메타데이터 옵션
   - metadata.max.age.ms : 메타데이터를 강제로 리프래시하는 간격. 기본값은 5분.
   - metadata.max.idle.ms : 프로듀서가 유휴상태일 경우 메타데이터를 캐시에 유지하는 기간. 예를 들어 프로듀서가 특정 토픽으로 데이터를 보낸 이후
                                     지정한 시간이 지나고 나면 강제로 메타데이터를 리프래시. 기본값은 5분

클라이언트 메타데이터가 이슈가 발생한 경우 (잘못된 파티션으로 요청 한 경우)
 - 카프카 클라이언트는 반드시 리더 파티션과 통신해야 한다. 만약 메타데이터가 현재의 파티션 상태에 맞게 리프래시되지 않은 상태에서 잘못된 
   브로커로 데이터를 요청하면 LEADER_NOT_AVAILABLE 익셉션이 발생한다. 
   이 에러는 클라이언트(프로듀서 또는 컨슈머)가 데이터를 요청한 브로커에 리더 파티션이 없는 경우 나타나며 대부분의 경우 메타데이터 리프래시 
   이슈로 발생한다. 이 에러가 자주 발생한다면 메타데이터 리프래시 간격을 확인하고 클라이언트가 정상적인 메타데이터를 가지고 있는지 확인해야한다.


Saas(Software-as-a-Service) 
 - 컨플루언트 클라우드 또는 AWS MSK는 대표적인 SaaS
 - 다양한 주변 생태계(ksqlDB, 모니터링 도구 등)를 옵션으로 제공

오픈 소스 카프카를 직접 설치하여 운영하는 경우
 - IaaS 또는 온프레미스 환경에서 카프카 클러스터를 설치하여 운영하는 것이 가증 흔한 운영 방식이다.
   카프카는 전송된 데이터를 모두 파일 시스템에 저장하고 대규모 데이터 통신이 일어나기 때문에 고성능의 하드웨어를 사용해야 한다.
   컨플루언트에서는 상용 환경의 카프카 클러스터 운영시 브로커의 하드웨어를 다음과 같이 설정하는 것을 추천한다.
   - 메모리 : 32GB 머신에 힙 메모리 6GB로 설정. 나머지는 OS의 페이지 캐시 영역으로 활용
   - CPU : 24core 머신 사용. 만약 SSL과 같은 보안 설정 사용할 경우 추가 더 높은 사용 필요
   - 디스크 : RAID 10으로 설정된 디스크 사용. NAS는 사용하면 안됨.
   - 네트워크 : 사용하는 데이터 통신량에 따라 다름.
   - 파일시스템 : XFS 또는 ext4

오픈 소스 카프카를 직접 설치하여 운영하는 경우 
항목               개발용 카프카 클러스터                     상용 환경 카프카 클러스터
----------------------------------------------------------------------------------------------
브로커 개수                 5개                                                      10개
메모리                    16GB(heap memory 6GB)                     32GB(heap memory 6GB)
CPU                              16core                                              24core
디스트                      사용량에 따라 달라짐                          사용량에 따라 달라짐 


클라우드 서비스 - 컨플루언트 클라우드, 컨플루언트 플랫폼
클라우드 서비스 - AWS MSK (Amazon Managed Streaming for Apache Kafka)              
                         AWS MSK는 AWS 인프라에서 카프카 클러스터를 생성, 업데이트, 삭제 등과 같은 운영 요소를 대시보드를 통해 제공. 

SaaS로 카프카 클러스터를 운영할 경우 장점
 - 인프라 관리의 효율성
    카프카 클러스터는 상용환경에서 최소 3대 이상의 서버로 운영한다. 클러스터를 담당하고 운영하는 개발자의 입장에서는 3대 이상의 서버를
    모니터링해야 한다는 것인데, SaaS를 사용 할 경우 인프라 운영관련 역할에서 자유로울 수 있다.
    브로커가 올라가는 서버는 자동으로 관리되기 때문이다. 

 - 모니터링 대시보드 제공
   카프카 클러스터를 직접 설치한다고해서 카프카를 잘 활용할 수 있는 것은 아니다. 브로커들이 제공하는 지표들을 수집하고 적재하고 대시보드화하여
   데이터를 시각화해야 카프카 클러스터를 효과적으로 운영하기 위해 필요한 설정들을 수정하고 적용할 수 있다.

 - 보안 설정
   보안 설정이 되지 않은 카프카 클러스터는 자물쇠가 달리지 않은 외양간과 같다.
   카프카 브로커는 SSL, SASL, ACL과 같이 불특정 다수의 침입을 막기 위해 다양한 종류의 보안 설정 방안을 제공하고 있다.
   Saas형 카프카에서는 접속 시 보안 설정을 기본으로 제공하고 있다. 클러스터 생성 시 보안 설정을 통해 인가된 사용자만 카프카 클러스터에 
   접근할 수 있다.

SaaS로 카프카 클러스터를 운영할 경우 단점
 - 서비스 사용 비용
   AWS MSK의 경우 3대의 브로커로 인스턴스을 구성하여 사용하면 시간당 1.5달라로 한 달에 1,080달러(한화 약 120만 원)의 비용이 발생한다.
   직접 서버를 발급하여 설치하고 운영하는 것이 AWS MSK를 사용하는 것에 비해 2배 이상 저렴하다.

 - 커스텀마이징의 제한 
   카프카 클러스터를 직접 운영하다 보면 서버의 최적화 옵션이나 카프카 브로커 옵션 같은 다양한 부분에서 사용자 설정이 들어간다.
   SaaS 서비스들은 인프라부터 애플리케이션 설치까지 모든 부분이 자동화 되어 있다.
 
 - 클라우드의 종속성 
   한번 선택한 SaaS형 카프카를 중단하지 않고 한 개의 서비스 업체에서 지속적으로 운영하는게 가장 좋은 방법이지만 현실에서는 다양한 이슈가 발생.


카프카 커맨드 라인 툴 
 - 카프카에서 제공하는 카프카 커맨드 라인 툴(command-line tool) CLI 들은 카프카를 운영할 때 가장 많이 접하는 도구.
   커맨드 라인 툴을 통해 카프카 브로커 운영에 필요한 다양한 명령을 내릴 수 있다. 
   커맨드 라인 툴을 통해 토픽 관련 명령을 실행할 때 필수 옵션과 선택 옵션이 있다. 선택 옵션은 지정하지 않을 시 브로커에 설정된 
   기본 설정값 또는 커맨드 라인 툴의 기본값으로 대체되어 설정된다. 그러므로 커맨드 라인 툴을 사용하기 전에 현재 브로커에 옵션이 어떻게 
   설정되어 있는지 확인 한 후에 사용하면 커맨드라인 툴 사용시 실수할 확률이 줄어든다.


로컬 카프카 설치 및 실행
1) 카프카 바이너리 파일 다운로드
 - https://kafka.apache.org/downloads
 - Bynary downloads: kafka_2.12-2.5.0.tgz
2) 카프카 바이너리 압축 해제
3) 주키퍼 실행
4) 카프카 바이너리 실행

주키퍼,카프카 1.8이상의 JDK 필요 JVM위에서 실행되기 때문에 
server.properties 는 config디렉토리에 존재 
브로커를 실행할때 여러 옵션 포함
log.dirs <-  파일시스템이 만들어지는 디렉토리 
num.partitions =3 <-- 기본적으로 만들어지는 파티션 개수 


카프카 브로커 실행전 주키퍼 실행 앙상블로 실행하는게 운영에서 사용한다. 3개이상의 서버에서 연동하는게 일반적이다. 
그래서 카프카 bin 프로젝트에 zookeper-server-start.sh 또는 .bat 파일 존재 

server.properties 수정 
listeners=PLAINTEXT://localhost:9092
advertised.listeners=PLAINTEXT://localhost:9092
위두개 주적을 풀어서 localhost 테스트 환경으로 변경하여 저장 
** 카프카 브로커가 로컬호스트에 있구나 라는 것을 다시 메타데이터를 받게 되고 이 메타데이터를 바탕으로 우리 카프카 클라이언트는 지속적으로
통신을 하게되는 옵션 

log.dirs=D:\kafka\kafka_2.12-2.5.0\data  <- 로저장 파일을 data 폴더에 저장하도록 설정 


======================================================================


주키퍼 실행 명령어 (Windows PowerShell 이용)
D:\kafka\kafka_2.12-2.5.0> 
.\bin\windows\zookeeper-server-start.bat config/zookeeper.properties

카프카 실행 명령어 (카프카 브로커)
D:\kafka\kafka_2.12-2.5.0> 
.\bin\windows\kafka-server-start.bat config/server.properties

브로커가 실행이 정상적으로 되었는지 확인 방법 
1. kafka 브로커 api 버전스라고 하는 쉘 스크립트로 확인 
D:\kafka\kafka_2.12-2.5.0> 
.\bin\windows\kafka-broker-api-versions.bat --bootstrap-server localhost:9092
카프카 기본적으로 9092 포트를 사용하게 된다. 
내부에서 설정된 다양한 옵션 값들에 대한 정보 표시됨 

2. kafka-topic 활용 
D:\kafka\kafka_2.12-2.5.0> 
.\bin\windows\kafka-topics.bat --bootstrap-server localhost:9092 --list
해당 브로커에대해서 토픽 리스트 정보를 조회


*** windows에서 hosts 수정 ***
C:\Windows\System32\drivers\etc

127.0.0.1 my-kafka 

추가저장

============================================================
kafka-topics.sh , bat 

hello.kafka 토픽처럼 카프카 클러스터 정보와 토픽 이름만으로 토픽을 생성할 수 있다. 클러스터 정보와 토픽 이름은 토픽을 만들기 위한 필수 값이다.
이렇게 만들어진 토픽은 파티션 개수, 복제 개수 등과 같이 다양한 옵션이 포함되어 있다. 

파티션 개수, 복제 개수, 토픽 데이터 유지 기간 옵션들을 지정하여 토픽을 생성하고 싶다면 다음과 같이 명령을 실행하면 된다. 생성된 토픽들의 이름을
조회하려면 --list 옵션을 사용한다. 

- 토픽 생성 

.\bin\windows\kafka-topics.bat --create --bootstrap-server my-kafka:9092 --topic hello.kafka

.\bin\windows\kafka-topics.bat --bootstrap-server my-kafka:9092 --topic hello.kafka --describe

.\bin\windows\kafka-topics.bat --create --bootstrap-server my-kafka:9092 --partitions 10 --replication-factor 1 --topic hello.kafka2 --config retention.ms=172800000

.\bin\windows\kafka-topics.bat --bootstrap-server my-kafka:9092 --list 

- 파티션 개수를 늘리기 위해서는 --alter 옵션을 사용하면 된다.
.\bin\windows\kafka-topics.bat --create --bootstrap-server my-kafka:9092 --topic test

.\bin\windows\kafka-topics.bat --bootstrap-server my-kafka:9092 --topic test --describe

.\bin\windows\kafka-topics.bat --bootstrap-server my-kafka:9092 --topic test --alter --partitions 4

.\bin\windows\kafka-topics.bat --bootstrap-server my-kafka:9092 --topic test --describe

Consumer의 데이터 처리량을 늘리기위해 리니얼하게 파티션을 늘린다. 

파티션 개수를 늘릴 수 있지만 줄일 수는 없다. 다시 줄이는 명령을 내리면 InvalidPartitionsException익셉션이 발생한다. 
분산 시스템에서 이미 분산된 데이터를 줄이는 방법은 매우 복잡하다. 삭제 대상 파티션을 지정해야할 뿐만 아니라 기존에 저장되어 있던 레코드를
분산하여 저장하는 로직이 필요하기 때문이다. 이때문에 카프카에서는 파티션을 줄이는 로직은 제공하지 않는다. 만약 피치못할 사정으로 파티션 개수를
줄여야 할 때는 토픽을 새로 만드는편이 좋다.


============================================================================================
kafka-configs.sh , bat 

토픽의 일부 옵션을 설정하기 위해서는 kafka-config.sh 명령어를 사용해야 한다. 
--alter 와 --add-config 옵션을 사용하여 min.insync.replicas 옵션을 토픽별로 설정할 수 있다. 

./bin/windows/kafka-configs.bat --bootstrap-server my-kafka:9092 --alter --add-config min.insync.replicas=2 --topic test

> 우리가 프로듀서로 데이터를 보낼때 그리고 컨슈머가 데이터를 읽을 때 워터마크 용도로 사용되고 얼마나 좀더 안전하게 데이터를 보내야 되는지에
대해서도  명확하게 설정할때 많이 사용

./bin/windows/kafka-topics.bat --bootstrap-server my-kafka:9092 --topic test --describe


브로커에 설정된 각종 기본값은 -- broker, --all, --describe 옵션을 사용하여 조회할 수 있다. 
./bin/windows/kafka-configs.bat --bootstrap-server my-kafka:9092 --broker 0 --all --describe
> 카프카 브로커0번에 대해서 옵션값들을 확일 수 있다. 

============================================================================================
kafka-concole-producer.sh

hello.kafka 토픽에 데이터를 넣을 수 있는 kafka-console-producer.sh 명령어를 실행 할 수 있다. 키보드로 문자를 작성하고 엔터 키를 누르면
별다른 응답 없이 메시지 값이 전송된다. *** 레코드가 생성 된다

./bin/windows/kafka-console-producer.bat --bootstrap-server my-kafka:9092 --topic hello.kafka
>hello   <-- 하나의 줄이 하나의 레코드로 전송된다.
>kafka
>0
>1
>2
>3
>4
>5
>6

메시지키는 null로 설정되어 라운드로빈으로 순차적으로 들어감

메시지 키를 가지는 레코드를 전송해 보자. 메시지 키를 가지는 레코드를 전송하기 위해 서는 몇가지 추가 옵션을 작성해야 한다.
key.separator를 선언하지 않으면 기본 설정은 Tab delimiter(\n)이므로 key.separator를 선언하지 않고 메시지를 보내려면 메시키 키를 작성하고 탭키를 누른 뒤 메시지 값을 작성하고 엔터를 누른다. 여기서는 명시적으로 확인하기 위해 콜롬(:)을 구분자로 선언한다.

./bin/windows/kafka-console-producer.bat --bootstrap-server my-kafka:9092 --topic hello.kafka --property "parse.key=true" --property "key.separator=:"
>key1:no1
>key2:no2
>key3:no3

parse.key=true 인경우 앞쪽은 키 , 뒷쪽은 값으로 입력가능 
메시지 키와 메시지 값이 포함된 레코드가 파티션에 전송됨
메시지 키와 메시지 값을 함께 전송한 레코드는 토픽의 파티션에 저장된다. 메시지 키가 null 인 경우에는 프로듀서가 파티션으로 전송할 때 
레코드 배치 단위(레코드 전송 묶음)로 라운드 로빈으로 전송한다.
메시지 키가 존재하는 경우에는 키의 해시값을 작성하여 존재하는 파티션 중 한개에 할당된다. 이로인해 메시지키가 동일한 경우에는 동일한 파티션으로 
전송된다.

동일한키가 한파티션에 들어가게된다. 컨슈머 입장에서는 특정데이터에대해서 순서를 보장할 수 있다. 
메시지키를 지정하면 토픽의 한파티션에 오프셋이 다르게 저장된다. 다른 파티션에는 해당 키가 저장되지 않는다. 


============================================================================================
kafka-console-consumer.sh, bat

hello.kafka 토픽으로 전송한 데이터는 kafka-console-consumer.sh 명령어로 확인할 수 있다. 이때 필수 옵션으로 --bootstrap-server에 카프카 클러스터 정보, --topic에 토픽 이름이 필요하다. 추가로 --from-beginning 옵션을 주면 토픽에 저장된 가장 처음 데이터부터 출력한다.

./bin/windows/kafka-console-consumer.bat --bootstrap-server my-kafka:9092 --topic hello.kafka --from-beginning

만약 레코드의 메시지 키와 메시지 값을 확인하고 싶다면 --property 옵션을 사용하면된다.
./bin/windows/kafka-console-consumer.bat --bootstrap-server my-kafka:9092 --topic hello.kafka --property print.key=true --property key.separator="-" --from-beginning

--max-messages 옵션을 사용하면 최대 컨슘 메시지 개수를 설정할 수 있다
./bin/windows/kafka-console-consumer.bat --bootstrap-server my-kafka:9092 --topic hello.kafka --property print.key=true --property key.separator="-" --from-beginning --max-messages 1 

--partition 옵션을 사용하면 특정 파티션만 컨슘할 수 있다. 
./bin/windows/kafka-console-consumer.bat --bootstrap-server my-kafka:9092 --topic hello.kafka --partition 2 --from-beginning

--group 옵션을 사용하면 컨슈머 그룹을 기반으로 kafka-console-consumer가 동작한다. 컨슈머 그룹이란 특정 목적을 가진 
컨슈머들을 묶음으로 사용하는 것을 뜻한다. 컨슈머 그룹으로 토픽의 레코드를 가져갈 경우 어느 레코드까지 읽었는지에 대한 데이터가 카프카 브로커에 
저장된다.

./bin/windows/kafka-console-consumer.bat --bootstrap-server my-kafka:9092 --topic hello.kafka --group hello-group --from-beginning

hello-group 이란 그룹은 브로커에가 현재 읽은 데이터까지 커밋을 한다. --consumer_offsets 카프카 토픽에 데이터가 저장된다.

그룹이 커밋한 오프셋을 조회하는건 아래와 같다
./bin/windows/kafka-topics.bat --bootstrap-server my-kafka:9092 --list
아~ consumer group 이란걸 사용하면 __consummer_offsets 가생기고 offset 관리하는구나 

============================================================================================
kafka-console-group.sh, bat

hello-group 이름의 컨슈머 그룹으로 생성된 컨슈머로 hello.kafka 토픽의 데이터를 가져갔다. 컨뮤서 그룹은 따로 생성하는 명령을 날리지 않고 
컨슈머를 동작할 때 컨슈머 그룹이름을 지정하면 이때 새로 생성된다. (오프셋 커밋을 날린다.)
생성된 컨슈머 그룹의 리스트는 kafka-consumer0group.sh 명령어로 확인할 수 있다. 

./bin/windows/kafka-consumer-groups.bat --bootstrap-server my-kafka:9092 --list

특정 그룹에 대해 더 자세한 내용은 

./bin/windows/kafka-consumer-groups.bat --bootstrap-server my-kafka:9092 --group hello-group --describe

--describe 옵션을 사용하면 해당 컨슈머 그룹이 어떤 토픽을 대상으로 레코드를 가져갔는지 상태를 확인 할 수 있다. 
파티션 번호, 현재까지 가져간 레코드의 오프셋, 파티션 마지막 레코드의 오프셋, 컨슈머 랙, 컨슈머 ID, 호스트를 알 수 있기 때문에 
컨슈머의 상태를 조회할 때 유용하다.
컨슈머 랙 = 파티션 마지막 레코드의 오프셋 - 가져간 레코드의 오프셋 의 차이다.

컨슈머 기능중 주요 기능중 하나는 오프셋 리셋! 기능이다 
특정 오프셋부터 다시 가져갈지 정하게 된다 

./bin/windows/kafka-consumer-groups.bat --bootstrap-server my-kafka:9092 --group hello-group --topic hello.kafka --reset-offsets --to-earliest --execute

./bin/windows/kafka-consumer-groups.bat --bootstrap-server my-kafka:9092 --topic hello-kafka --group hello-group

오프셋 리셋 종류 
--to-earliest : 가장 처음 오프셋(작은 번호)으로 리셋 
--to-latest : 가장 마지막 오프셋(큰 번호)으로 리셋
--to-current : 현 시점 기준 오프셋으로 리셋
--to-datetime{YYYY-MM-DDTHH:mmSS.sss} : 특정 일시로 오프셋 리셋(레코드 타임스탬프 기준)
--to-offset {long} : 특정 오프셋으로 리셋
--shift-by {+/- long} : 현재 컨슈머 오프셋에서 앞뒤로 옮겨서 리셋

./bin/windows/kafka-consumer-groups.bat --bootstrap-server my-kafka:9092 --topic hello-kafka --group hello-group --reset-offsets --to-earliest --execute
처음부터 재처리함 

./bin/windows/kafka-console-consumer.bat --bootstrap-server my-kafka:9092 --topic hello.kafka --group hello-group 
명령어를 치는순간 프로듀서가 생성한 데이터를 처음부터 마지막까지 재처리 하게된다. 


============================================================================================
그외 커맨드 툴

kafka-producer-perf-test.sh는 카프카 프로듀서로 퍼포먼스를 측정할 때 사용된다. 

./bin/windows/kafka-producer-perf-test.bat --producer-props bootstrap.servers=my-kafka --topic hello.kafka --num-records 10 --throughput 1 --record-size 100 --print-metric

특정 부트스트랩 특정포트로 몇개의 레코드를 스로풋으로 특정 토픽으로 레코드를 생성할때 네트워크 측정

kafka-consumer-perf-test.sh는 카프카 컨슈머로 퍼포먼스를 측정할 때 사용된다. 
카프카 브로커와 컨슈머간의 네트워크 체크를 할 때 사용 

./bin/windows/kafka-consumer-perf-test.bat --bootstrap-server my-kafka:9092 --topic hello.kafka --messages 10 --show-detailed-stats


kafka-reassign-partitions.sh 
파티션이 한곳에 모였다면 각각의 브로커에 파티션을 재분배 하게 한다. 
kafka-reassign-partitions.sh를 사용하면 리더 파티션과 팔로워 파티션이 위치를 변경할 수 있다. 카프카 브로커에는 auto.leader.rebalance.enable 옵션이
있는데 이 옵션의 기본값은 true로써 클러스터 단위에서 리더 파티션을 자동 리밸런싱하도록 도와준다. 브로커의 백그라운드 스레드가 일정한 간격으로
리더의 위치를 파악하고 필요시 리더 리밸런싱을 통해 리더의 위치가 알맞게 배분된다.

./bin/windows/kafka-reassign-partitions.bat --zookeeper my-kafka:2181


kafka-delete-record.sh 
kafka-dum-log.sh <- 로그를 확인하기 위해서 
./bin/windows/kafka-dump-log.sh --files data/hello.kafka-0/00000000000000000000.log  -- deep-iteration


============================================================================================

토픽을 생성하는 두가지 방법
토픽을 생성하는 상황은 크게 2가지 
1. 카프카 컨슈머 또는 프로듀서가 카프카 브로커에 생성되지 않은 토픽에 대해 데이터를 요청할 때 
2. 커맨드 라인 툴로 명시적으로 토픽을 생성 할 떄 

토픽마다 처리되어야 하는 데이터의 특성이 다르기 때문에 토픽을 생성할 때는 데이터 특성에 따라 옵션을 다르게 설정 할 수 있따.
예를 들어, 동시 데이터 처리량이 많아야 하는 토픽의 경우 파티션의 개수를 100으로 설정할 수 있다. 
단기간 데이터 처리만 필요한 경우에는 토픽에 들어온 데이터의 보관기간 옵션을 짧게 설정할 수도 있다. 그러므로 토픽에 들어오는 데이터양과
병렬로 처리되어야 하는 용량을 잘파악하여 생성하는 것이 중요하다.


============================================================================================

카프카 브로커와 로컬 커맨드 라인 툴 버전을 맞춰야 하는 이유
카프카 브로커로 커맨드 라인 툴 명령을 내릴 때 브로커의 버전과 커맨드 라인 툴 버전을 반드시 맞춰서 사용하는 것을 권장한다.
브로커의 버전이 업그레이드 됨에 따라 커맨드 라인 툴의 상세 옵션이 달라지기 때문에 버전 차이로 인해 명령이 정상적으로 실행되지 않을 수 있다.
x.x.x 바이너리 패키지에 들어있는 카프카 커맨드라인 툴을 이용한다. 

============================================================================================

카프카 프로듀서 
카프카에서 데이터의 시작점은 프로듀서이다. 프로듀서 애플리케이션은 카프카에 필요한 데이터를 선언하고 브로커의 특정 토픽의 파티션에 전송한다.
프로듀서는 데이터를 전송할 때 리더 파티션을 가지고 있는 카프카 브로커와 직접 통신한다.
프로듀서는 카프카 브로커로 데이터를 전송할 때 내부적으로 파티셔너, 배치 생성 단계를 거친다.

프로듀서 내부 구조 
 - ProducerRecord : 프로듀서에서 생성하는 레코드. 오프셋은 미포함.
 - send() : 레코드 전송 요청 메서드.
 - Partitioer : 어느 파티션으로 전송할지 지정하는 파티셔너. 기본값으로 DefaultPartitioner로 설정됨
 - Accumulator : 배치로 묶어 전송할 데이터를 모으는 버퍼 

프로듀서의 기본 파티셔너 
프로듀서API를 사용하면 'UniformStickyPartitioner'와 'RoundRobinPartitioner' 2개 파티셔너를 제공 한다.
카프카 클라이언트 라이브러리 2.5.0 버전에서 파티셔너를 지정하지 않은 경우 UniformStickyPartitioner가 파티셔너로 기본 설정된다.

 메시지 키가 있을 경우 동작 
 - UniformStickyPartitionerd 와 RoundRobinPartitioner 둘 다 메시지 키가 있을 때는 메시지 키의 해시값과 파티션을 매칭하여 레코드를 전송.
 - 동일한 메시지 키가 존재하는 레코드는 동일한 파티션 번호에 전달한다. 
 - 만약 파티션 개수가 변경될 경우 메시지 키와 파티션 번호 매칭은 깨지게 된다. 

 메시지 키가 없을 경우 동작 
 메시지 키가 없을 때는 파티션에 최대한 동일하게 분배하는 로직이 들어있는데 UniformStickyPartitionerd 는 RoundRobinPartitioner의 단점을 개선
 디폴트값이 UniformStickyPartitionerd 다 

  RoundRobinPartitioner
  - ProducerRecord가 들어오는 대로 파티션을 순회하면서 전송.
  - 어큐뮬레이터에서 묶이는 정도가 적기 때문에 전송 성능이 낮음.

  UniformStickyPartitionerd
  - 어큐뮬레이터에서 레코드들이 배치로 묶일 때까지 기다렸다가 전송.
  - 배치로 묶일 뿐 파티션을 순회하면서 보내기 때문에 모든 파티션에 분배되어 전송됨
  - RoundRobinPartitioner에 비해 향상된 성능을 가짐 

  프로듀서의 커스텀 파티셔너 
  카프카 클라이언트 라이브러리에서는 사용자 지정 파티셔너를 생성하기 위한 Partitioner 인터페이스를 제공한다.
  Partitioner 인터페이스를 상속받은 사용자 정의 클래스에서 메시지 키 또는 메시지값에 따른 파티션 지정 로직을 적용할 수 있다.
  파티셔너를 통해 파티션이 지정된 데이터는 어큐뮬레이터에 버퍼로 쌓인다. 센더(sender) 스레드는 어큐뮬레이터세 쌓인 배치 데이터를 가져가
  카프카 브로커로 전송한다.

  프로듀서 주요 옵션(필수 옵션)
  - bootstrap.servers : 프로듀서가 데이터를 전송할 대상 카프카 클러스터에 속한 브로커의 호스트 이름:포트를 1개이상 작성한다. 
                             2개 이상 브로커 정보를 입력하여 일부 브로커에 이슈가 발생하더라도 접속하는 데에 이슈가 없도록 설정 가능
  - key.serializer : 레코드의 메시지 키를 직렬화하는 클래스를 지정한다.
  - value.serializer : 레코드의 메시지 값을 직렬화하는 클래스를 지정한다. 

  특정 토픽에 프로듀서가 직렬화를 어떤 타입으로 했는지 관리해야 컨슈머가 역질렬화를 통해서 데이터를 얻을수 있다. 
  다만 string 타입의 데이터가 아니면 kafka-console-consumer 스크립트로 확인이 불가능하다.
  
  프로듀서 주요 옵션(선택 옵션) 버전별 상이 
  - acks : 프로듀서가 전송한 데이터가 브로커들에 정상적으로 저장되었는지 전송 성공 여부를 확인 하는 데에 사용하는 옵션. 0, 1, -1(all) 중 
             하나로 설정할 수 있다. 기본 값은 1이다.
  - linger.ms : (어큐뮬레이터)배치를 전송하기 전까지 기다리는 최소 시간. 기본값은 0 
  - retries : 브로커로부터 에러를 받고 난 뒤 재전송을 시도하는 횟수를 지정한다. 기본값은 2147483647 이다.
  - max.in.flight.requests.per.connection : 한 번에 요청하는 최대 커넥션 개수. 설정된 값만큰 동시에 전달 요청을 수행한다. 기본값을 5이다.
  - partitioner.class : 레코드를 파티션에 전송할 때 적용하는 파티셔너 클래스를 지정한다. 
                            기본값은 org.apache.kafka.clients.producer.internals.DefaultPartitioner 이다
  - enable.idempotence : 멱등성 프로듀서로 동작할지 여부를 설정한다. 기본값은 false 이다.
  - transactional.id : 프로듀서가 레코드를 전송할 때 레코드를 트랜잭션 단위로 묶을지 여부를 설정한다. 기본값은 null이다.

 ISR(In-Sync-Replicas)와 acks 옵션
 ISR은 리더 파티션과 팔로워 파티션이 모두 싱크가 된 상태를 뜻한다. 복제 개수가 2인 토픽을 가정해 보자. 
 이 토픽에는 리더 파티션 1개와 팔로워 파티션이 1개가 존재할 것이다. 리더 파티션에 0부터 3의 오프셋이 있다고 가정할 떄,
 팔로워 파티션에 동기화가 완료되려면 0부터 3까지 오프셋이 존재해야 한다. 
 동기화가 완료됐다는 의미는 리더 파티션의 모든 데이터가 팔로워 파티션에 복제된 상태를 말하기 때문이다.

 ISR이라는 용어가 나온 이유는 팔로워 파티션이 리더파티션으로부터 데이터를 복제하는 데에 시간이 걸리기 때문이다.
 프로듀서가 특정 파티션에 데이터를 저장하는 작업은 리더 파티션을 통해 처리한다. 이때 리더 파티션에 새로운 레코드가 추가되어 오프셋이
 증가하면 팔로워 파티션이 위치한 브로커는 리더 파티션의 데이터를 복제한다. 리더 파티션에 데이터가 적재된 이후 팔로워 파티션이 복제하는 시간차
 때문에 리더 파티션과 팔로워 파티션 간에 오프셋 차이가 발생한다.

 acks
 카프카 프로듀서의  acks옵션은 0, 1, all(또는 -1) 값을 가질 수 있다. 이 옵션을 통해 프로듀서가 전송한 데이터가 카프카 클러스터에 얼마나 신뢰성 
 높게 저장할지 지정할 수 있다. 그리고, acks옵션에 따라 성능이 달라질 수 있으므로 acks옵션에 따른 카프카의 동작 방식을 상세히 알고 설정해야 한다.
 복제 개수가 1인경우 acks옵션에 따른 성능 변화는 크지 않다. 

  - acks=0
  acks를 0으로 설정하는 것은 프로듀서가 리더 파티션으로 데이터를 전송했을 때 리더 파티션으로 데이터가 저장되었는지 확인하지 않는다는 뜻.
  리더 파티션은 데이터가 저장된 이후에 데이터가 몇 번째 오프셋에 저장되었는지 리턴하는데, acks가 0으로 설정되어 있다면 프로듀서는 리더 파티션에 
  데이터가 저장되었는지 여부에 대한 응답 값을 받지 않는다. 데이터의 전송 속도는 acks를 1 또는 all로 했을 경우보다 훨씬 빠르다.
  데이터가 일부 유실이 발생하더라도 전송 속도가 중요한 경우에는 이 옵션값을 사용하면 좋다.

  - acks=1
  acks를 1로 설정할 경우 프로듀서는 보낸 데이터가 리더 파티션에만 정상적으로 적재되었는지 확인한다.
  만약 리더 파티션에 정상적으로 적재되지 않았다면 리더 파티션에 적재될 때까지 재시도할 수 있다.
  그러나 리더 파티션에 적재되었음을 보장하더라도 데이터는 유실될 수 있다. 왜냐하면 복제 개수를 2 이상으로 운영할 경우 
  리더 파티션에 적재가 완료되어도 팔로워 파티션에는 아직 데이터가 동기화되지 않을 수 있는데, 팔로워 파티션이 데이터를 복제하기 직전에
  리더 파티션이 있는 브로커에 장애가 발생하면 동기화되지 못한 데이터가 유실될 수 있기 때문이다.

  - acks=-1(all)
  acks를 all 또는 -1로 설정할 경우 프로듀서는 보낸 데이터가 리더 파티션과 팔로워 파티션에 모두 정상적으로 적재되었는지 확인한다. 
  리더 파티션뿐만 아니라 팔로워 파티션까지 데이터가 적재되었는지 확인하기 때문에 0 또는 1 옵션보다도 속도가 느리다. 
  그럼에도 불구하고 팔로워 파티션에 데이터가 정상 적재되었는지 기다리기 때문에 일부 브로커에 장애가 발생하더라도 프로듀서는 안전하게
  데이터를 전송하고 저장 할 수 있음을 봉장할 수 있다. acks를 all로 설정할 경우에는 토픽 단위로 설정 가능한 
  min.insync.replicas 옵션값에 따라 데이터의 안정성이 달라진다.
  
  min.insync.replicas
  min.insync.replicas 옵션은 프로듀서가 리더 파티션과 팔로워 파티션에 데이터가 적재되었는지 확인하기 위해 최소 ISR그룹의 파티션 개수이다.
  예를 들어, min.insync.replicas의 옵션값이 1이라면 ISR 중 최소 1개 이상의 파티션에 데이터가 적재되었음을 확인하는 것이다.
  이 경우 acks를 1로 했을 때와 동일한 동작을 하는데, 왜냐하면 ISR 중 가장 처음 적재가 완료되는 파티션은 리더 파티션이기 때문이다.

  acks=-1(all), min.insync.replicas=2
  min.insync.replicas의 옵션값을 2로 설정했을 때부터 acks를 all로 설정하는 의미가 있다. 이 경우 ISR의 2개 이상의 파티션에 데이터가 정상 적재되었음을 
  확인한다는 뜻이다. ISR의 2개 이상의 파티션에 적재되었음을 확인한다는 뜻은 적어도 리더 파티션과 1개의 팔로워 파티션에 데이터가 정상적으로 적재
  되었음을 보장한다. 실제 카프카 클러스터를 운영하면서 브로커가 동시에 2개가 중단되는 일은 극히 드물기 때문에 리더 파티션과 팔로워 파티션 중
  1개에 데이터가 적재 완료되었다면 데이터는 유실되지 않는다고 볼 수 있다.

 카프카 브로커와 연동하기 위한 프로듀서를 사용하기 위해서는 라이브러리를 사용해야 한다. 아파치 카프카는 공식 라이브러리 자바 라이브러리를 
 지원한다. 

  Maven 프로젝트에 클라이언트 종송 항목 추가 
https://docs.redhat.com/ko/documentation/red_hat_streams_for_apache_kafka/2.5/html/developing_kafka_client_applications/assembly-kafka-clients-maven-str#proc-adding-clients-dependency-str
  

<dependencies>
    <dependency>
        <groupId>org.apache.kafka</groupId>
        <artifactId>kafka-clients</artifactId>
        <version>2.5.0</version>
    </dependency>
</dependencies>


필수 라이브러리 선택 New Spring Starter Project Dependencies 
Spring Boot DevTools 
Spring Web 
LomBok 

  Maven poject로 토이프로젝트 진행 

============================================================================================
카프카 컨슈머

프로듀서가 전송한 데이터는 카프카 브로커에 적재된다. 컨슈머는 적재된 데이터를 사용하기 위해 브로커로부터 데이터를 가져와서 필요한 처리를 한다.
예를 들어, 마케팅 문자를 고객에게 보내는 기능이 있다면 컨슈머는 토픽으로부터 고객 데이터를 가져와서 문자 발송 처리를 하게 된다.

컨슈머 내부 구조 

카프카                             poll()
클러       ------> Fetcher  ---------->  ConsumerRecords (커밋과정이 있음)
스터               completedFetches

 - Fetcher : 리더 파티션으로부터 레코드들을 미리 가져와서 대기.
 - poll() : Fetcher에 있는 레코드들을 리턴하는 레코드.
 - ConsumerRecords : 처리하고자 하는 레코드들의 모음. 오프셋이 포함되어 있다.


 컨슈머 그룹
 컨슈머 그룹으로 운영하는 방법은 컨슈머를 각 컨슈머 그룹으로부터 격리된 환경에서 안전하게 운영할 수 있도록 도와주는 카프카의 득특한 방식.
 특정 토픽에 대해서 어떤 목적에 따라 데이터를 처리하는 컨슈머들을 묶은 그룹이 바로 컨슈머 그룹이라고 볼 수 있다. 
 컨슈머 그룹으로 묶인 컨슈머들은 토픽의 1개 이상 파티션들에 할당되어 데이터를 가져갈 수 있다. 컨슈머 그룹으로 묶인 컨슈머가 토픽을 구독해서 
 데이터를 가져갈떄, 1개의 파티션은 최대 1개의 컨슈머에 할당 가능하다. 
 그리고 1개 컨슈머는 여러 개의 파티션에 할당될 수 있다. 이러한 특징으로 컨슈머 그룹의 컨슈머 개수는 가져가고자 하는 토픽의 파티션 개수보다
 같거나 작아야 한다.

 컨슈머 그룹의 컨슈머가 파티션 개수보다 많을 경우 
 만약 4개의 컨슈머로 이루어진 컨슈머 그룹으로 3개의 파티션을 가진 토픽에서 데이터를 가져가기 위해 할당하면 1개의 컨슈머는 파티션을 
 할당받지 못하고 유휴 상태로 남게 된다. 파티션을 할당받지 못한 컨슈머는 스레드만 차지하고 실질적인 데이터 처리를 하지 못하므로 애플리케이션
 실행에 있어 불필요한 스레드로 남게 된다.

 컨슈머 그룹을 활용하는 이유
 운영 서버의 주요 리소스인 CPU, 메모리 정보를 수집하는 데이터 파이프라인을 구축한다고 가정해보자.
 실시간 리소스를 시간순으로 확인하기 위해서 데이터를 엘라스틱서치에 저장하고 이와 동시에 대용량 적재를 위해 하둡에 적재할 것이다.
 만약 카프카를 활용한 파이프라인이 아니라면 서버에서 실행되는 리소스 수집 및 전송 에이전트는 수집한 리소스를 엘라스틱서치와 하둡에 적재하기
 위해 동기적으로 적재를 요청 할 것이다. 이렇게 동기로 실행되는 에이전트는 엘라스틱서치 또는 하둡 둘 중 하나에 장애가 발생한다면 더는 적재가 
 불가능할 수 있다.
 
 카프카는 이러한 파이프라인을 운영함에 있어 최종 적재되는 저장소의 장애에 유연하게 대응할 수 있도록 각기 다른 저장소에 저장하는
 컨슈머를 다른 컨슈머 그룹으로 묶음으로써 각 저장소의 장애에 격리되어 운영 할 수 있다.
 따라서 엘라스틱서치의 장애로 인해 더는 적재가 되지 못하더라도 하둡으로 데이터를 적재하는 데이는 문제가 없다.
 엘라스틱서치의 장애가 해소되면 엘라스틱서치로 적재하는 컨슈머의 컨슈머 그룹은 마지막으로 적재 완료한 데이터 이후부터 다시 적재를 수행하여
 최종적으로 모두 정상화될 것이다.

 리밸런싱 (카프카 failover 방식)
 컨슈머그룹으로 이루어진 컨슈머들 중 일부 컨슈머에 장애가 발생하면, 장애가 발생한 컨슈머에 할당된 파티션은 장애가 발생하지 않은 컨슈머에
 소유권이 넘어간다. 이러한 과정을 리밸런싱(rebalancing)이라고 부른다. 리밸런싱을 크게 두가 지 상황에서 일어나느데, 첫번째는 컨슈머가 추가되는
 상황이고 두번째는 컨슈머가 제외되는 상황이다. 이슈가 발생한 컨슈머를 컨슈머 그룹에서 제외하여 모든 파티션이 지속적으로 데이터를 처리할 수 있도록
 가용성을 높여준다. 리밸런싱은 컨슈머가 데이터를 처리하는 도중에 언제든지 발생할 수 있으므로 데이터 처리 중 발생한 리벌런싱에 대응하는 코드를 
 작성해야 한다.
 리스너로 두가지 메서드로 제공(컨슈머가 추가, 제거 되는 상황에 대해서)

 커밋
------브로커--------
|        토픽          |                    컨슈머 그룹 
|  파티션|0|1|2|3   |  ------------>   컨슈머 0
|--------------------|    poll()              |
                                      <---------| commit()           

 컨슈머는 카프카 브로커로부터 리더 파티션에 데이터를 어디까지 가져갔는지 커밋(commit)을 통해 기록한다. 
 특정 토픽의 파티션을 어떤 컨슈머 그룹이 몇 번째 가져갔는지 카프카 브로커 내부에서 사용되는 내부 토픽(__consumer_offsets)에 기록된다.
 컨슈머 동작 이슈가 발생하여 __consumer_offsets 토픽에 어느 레코드까지 읽어갔는지 오프셋 커밋이 기록되지 못했다면 데이터 처리의 중복이 발생할
 수 있다. 그러므로 데이터 처리의 중복이 발생하지 않게 하기 위해 서는 컨슈머 애플리케이션이 오프셋 커밋을 정상적으로 처리했는지 검증해야만 한다.
 
 Assignor
 컨슈머와 파티션 할당 정책은 컨슈머의 어싸이너에 의해 결정된다.
 카프카에서는 RangeAssignor, RoundRobinAssignor, StickyAssignor를 제공한다. 
 - RangeAssignor : 각 토픽에서 파티션을 숫자로 정렬, 컨슈머를 사전 순서로 정렬하여 할당.
 - RoundRobinAssignor : 모든 파티션을 컨슈머에서 번갈아가면서 할당 .
 - StickyAssignor를 : 최대한 파티션을 균등하게 배분하면서 할당.

 컨슈머그룹은 토픽과 1 : 1 로 운영하는게 일반적이다.
 
 컨슈머 주요 옵션 소개 (필수 옵션)
 - bootstrap.servers : 프로듀서가 데이터를 전송할 대상 카프카 클러스터에 속한 브로커의 호스트 이름:포트를 1개이상 작성한다.
                             2개 이상 브로커 정보를 입력하여 일부 브로커에 이슈가 발생하더라도 접속하는 데에 이슈가 없도록 설정 가능하다.
 - key-deserializer : 레코드의 메시지 키를 역질렬화하는 클래스를 지정한다.
 - value-deserializer : 레코드의 메시지 값을 역질렬화하는 클래스를 지정한다.

 컨슈머 주요 옵션 (선택 옵션)
 - group.id : 컨슈머 그룹 아이디를 지정한다. subscribe() 메서드로 토픽을 구독하여 사용할 때는 이 옵션을 필수로 넣어야 한다. 기본값은 null 이다
 - auto.offset.reset : 컨슈머 그룹이 특정 파티션을 읽을 때 저장된 컨슈머 오프셋이 없는 경우 어느 오프셋부터 읽을지 선택하는 옵션이다.
                           이미 컨슈머 오프셋이 있다면 이 옵션값은 무시된다. 기본값은 latest이다.
 - enable.auto.commit : 자동 커밋으로 할지 수동 커밋으로 할지 선택한다. 기본값은 true이다.
 - max.poll.records : poll() 메서드를 통해 반환되는 레코드 개수를 지정한다. 기본값은 500이다. 
 - session.timeout.ms : 컨슈머가 브로커와 연결이 끊기는 최대 시간이다. 기본값은 10000(10초)이다.
 - heartbeat.interval.ms : 하트비트를 전송하는 시간 간격이다. 기본값은 3000(3초)이다.
 - max.poll.interval.ms : poll()메서드를 호출하는 간격의 최대 시간. 기본값은 300000(5분)이다.
 - isolation.level : 트랜잭션 프로듀서가 레코드를 트랜잭션 단위로 보낼 경우 하용한다.

 auto.offset.reset (컨슈머의 선택 옵션)
 컨슈머 그룹이 특정 파티션을 읽을 때 저장된 컨슈머 오프셋이 없는 경우 어느 오프셋부터 읽을지 선택하는 옵션이다.
 이미 컨슈머 오프셋이 있다면 이 옵션값은 무시한다. 이 옵션을 latest, earliest, none 중 1개를 설정할 수 있따.
 - latest : 설정하면 가장 높은(가장 최근에 넣은) 오프셋부터 읽기 시작한다.
 - earliest : 설정하면 가장 낮은(가장 오래전에 넣은) 오프셋부터 읽기 시작한다.
 - none : 설정하면 컨슈머 그룹이 커밋한 기록이 있는지 찾아본다. 만약 커밋 기록이 없으면 오류를 반환하고, 커밋기록이 있다면 기존 커밋 기록 이후 
             오프셋부터 읽기 시작한다. 기본값은 latest이다.

 멀티스레트 컨슈머
 기본적으로 카프카는 1thread 에 1consumer 이다

 카프카는 처리량을 늘리기 위해 파티션과 컨슈머 개수를 늘려서 운영할 수 있다. 
 파티션을 여러개로 운영하는 경우 데이터를 병렬처리하기 위해서 피티션 개수와 컨슈머 개수를 동일하게 맞추는 것이 가장 좋은 방법이다.
 토픽의 파티션은 1개 이상으로 이루어져 있으며 1개의 파티션은 1개 컨슈머가 할당되어 데이터를 처리할 수 있다.
 파티션 개수가 n개 라면 동일 컨슈머 그룹으로 묶인 컨슈머 스레드를 최대 n개 운영할 수 있따.
 그러므로 n개의 스레드를 가진 1개의 프로세스를 운영하거나 1개의 스레드를 가진 프로세스를 n개 운영하는 방법도 있다.

 컨슈머 랙 **** 아주중요 모니터링 시스템이 아주아주 필수!
 컨슈머 랙(LAG)은 파티션의 최신 오프셋(LOG-END-OFFSET)과 컨슈머 오프셋(CURRENT-OFFSET) 간의 차이다.
 프로듀서는 계속해서 새로운 데이터를 파티션에 저장하고 컨슈머는 자신이 처리할 수 있는 만큼 데이터를 가져간다.
 컨슈머 랙은 컨슈머가 정상 동작하는지 여부를 확인할 수 있기 때문에 컨슈머 애플리케이션을 운영한다면 필수적으로 모니터링해야 하는 지표이다.

 컨슈머 랙은 컨슈머 그룹과 토픽, 파티션별로 생성된다. 1개의 토픽에 3개의 파티션이 있고 1개의 컨슈머 그룹이 토픽을 구독하여 데이터를 가져가면
 컨슈머 랙은 총 3개이다. 

 프로듀서와 컨슈머의 데이터 처리량에 따라서 LAG 발생한다
 프로듀서가 보내는 데이터양이 컨슈머의 데이터 처리량보다 크다면 컨슈머 랙은 늘어난다.
 반대로 프로듀서가 보내는 데이터양이 컨슈머의 데이터 처리량보다 적으면 컨슈머 랙은 줄어들고 최솟값은 0으로 지연이 없음을 뜻한다.

 컨슈머 랙을 모니터링 하는 것은 카프카를 통한 데이터 파이프라인을 운영하는데에 핵심!!적인 역할을 한다.
 컨슈머 랙을 모니터링함으로써 컨슈머의 장애를 확인할 수 있고 파티션 개수를 정하는 데에 참고할 수 있기때문이다.
 예를 들어, 네비게이션의 사용자 데이터를 전송하는 프로듀서가 있다고 가정 할떄, 추석 설날과 같이 네비게이션 사용량이 많을 때 프로듀서가 
 카프카 클러스터로 전송하는 데이터양은 평소와는 확연히 다르게 올라간다.
 반면, 컨슈머는 프로듀서가 전송하는 데이터양이 늘어나더라도 최대 처리량은 한정되어 있기 때문에 추석과 설날에는 컨슈머 랙이 발생할수 있다.
 이러한 경우에는 지연을 줄이기 위해 일시적으로 파티션 개수와 컨슈머 개수를 늘려서 병렬처리량을 늘리는 방법을 사용할 수 있다.

 프로듀서의 데이터양이 늘어날 경우에는 컨슈머 랙이 늘어날수있따. 이경우 파티션 개수와 컨슈머 개수를 늘려 병렬처리량을 늘려 컨슈머 랙을 
 줄 일 수 있다. 컨슈머의 개수를 1개에서 2개로 늘림으로써 컨슈머의 데이터 처리량을 2배로 늘릴수 있다.
 
 프로듀서의 데이터양이 일정함에도 불구하고 컨슈머의 장애로 인해 컨슈머 랙이 증가할 수도 있다. 컨슈머는 파티션 개수만큼 늘려서 
 병렬처리하며 파티션마다 컨슈머가 할당되어 데이터를 처리한다. 예를 들어, 2개의 파티션으로 구성된 토픽에 2개의 컨슈머가 각각 할당되어 데이터를 
 처리한다고 가정해볼때 프로듀서가 보내는 데이터양은 동일한데 파티션 1번의 컨슈머 랙이 늘어나는 상황이 발생한다면 1번 파티션에 할당된 컨슈머에
 이슈가 발생했음을 유츄할 수 있다.

 컨슈머 랙을 확인하는 모니터링하는 방법 3가지 
 - kafka-consumer-group.sh 명령어를 사용하면 컨슈머 랙을 포함한 특정 컨슈머 그룹의 상태를 확인 할 수 있따.
   컨슈머랙을 확인하기 위한 가장 기초적인 방법, 명령어를 통해 컨슈머랙을 확인하는 방법은 일회성에 그치고 지표를 지속적으로 기록하고 모니터링
   하기에는 부족하다. 그렇기 때문에 스크립트 명령어를 통한 확인은 테스트용으로 확인을 주로 한다.

 - metrics() 메서드 사용
  컨슈머 애플리케이션에서 KafkaConsumer 인스턴스의 metrics() 메서드를 활용하면 컨슈머 랙 지표를 확인 할 수 있다.
  컨슈머 인스턴스가 제공하는 컨슈머 랙 관련 모니터링 지표는 3가지로 records-lag-max, record-lag, record-lag-avg 이다.
  - 컨슈머가 정상 동작할 경우에만 확인 할 수 있다. metrics() 메서드는 컨슈머가 정상적으로 실행될 경우만 호출 가능하다.
    그렇기 때문에 만약 컨슈머 애플리케이션이 비정상적으로 종료되면 더는 컨슈머 랙을 모니터링 할 수 없다.

  - 모든 컨슈머 애플리케이션에 컨슈머 랙 모니터링 코드를 중복해서 작성해야 한다. 컨슈머 애플리케이션을 여러 종류로 운영할 경우 각기 다른
    컨슈머 애플리케이션에 metrics() 메서드를 호출하여 컨슈머 랙을 수집하는 로직을 중복해서 넣어야 한다. 특 정 그룹에 해당하는 애플리케이션이 
    수집하는 컨슈머 랙은 자기 자신 컨슈머 그룹에 대한 컨슈머 랙만 한정되기 때문이다.
  - 컨슈머 랙을 모니터링하는 코드를 추가할 수 없는 카프카 서드파티 애플리케이션의 컨슈머랙 모니터링 불가능하다. 

 - 외부 모니터링 툴 사용
 컨슈머 랙을 모니터링하는 가장 최선의 방법은 외부 모니터링 툴을 사용하는 것이다. 데이터 독(Datadog), 
 컨플루언트 컨트롤 센터(Confluent Control Center) 와 같은 카프카 클러스터 종합 모니터링 툴을 사용하면 카프카 운영에 필요한 다양한 지표를 모니터링 
 할 수 있다. 모니터링 지표에는 컨슈머 랙도 포함되어 있기 때문에 클러스터 모니터링과 컨슈머랙을 함께 모니터링하기에 적합하다.
 컨슈머 랙 모니터링만을 위한 툴로 오픈소스로 공개되어 있는 버로우(Burrow)가 있다.

 카프카 버로우 (링크드인 개발 오픈소스)
 버로우는 링크드인에서 개발하여 오픈소스로 공개한 컨슈머 랙 체크 툴로서 REST API를 통해 컨슈머 그룹 별로 컨슈머 랙을 확인할 수 있다.
 외부 모니터링 툴을 사용하면 카프카 클러스터에 연결된 모든 컨슈머, 토픽들의 랙 정보를 한번에 모니터링 할 수 있다는 장점이 있따.
 또한, 모니터링 툴들은 틀러스터와 연동되어 컨슈머의 데이터 처리와는 별개로 지표를 수집하기 때문에 데이터를 활용하는 프로듀서나 컨슈머의 동작
 에 영향을 미치지 않는다는 장점도 있다.
 버로우는 다수의 카프카 클러스터를 동시에 연결하여 컨슈머 랙을 확인한다. 기업 환경에서는 카프카 클러스터를 2개 이상으로 구축하고 운영하는 
 경우가 많기 때문에 한번의 설정으로 다수의 카프카 클러스터 컨슈머 랙을 확인할 수 있다는 장점이 있다.

요청 메서드	호출 경로	설명
GET	/burrow/admin	버로우 헬스 체크
GET	/v3/kafka	버로우와 연동 중인 카프카 클러스터 리스트
GET	/v3/kafka/{클러스터 이름}	클러스터 정보 조회
GET	/v3/kafka/{클러스터 이름}/consumer	클러스터에 존재하는 컨슈머 그룹 리스트
GET	/v3/kafka/{클러스터 이름}/topic	클러스터에 존재하는 토픽 리스트
GET	/v3/kafka/{클러스터 이름}/consumer/{컨슈머 그룹 이름}	컨슈머 그룹의 컨슈머 랙, 오프셋 정보 조회
GET	/v3/kafka/{클러스터 이름}/consumer/{컨슈머 그룹 이름}/lag	컨슈머 그룹의 파티션 정보, 상태, 컨슈머 랙 조회
GET	/v3/kafka/{클러스터 이름}/topic/{토픽 이름}	토픽 상세 조회
https://zave7.github.io/kafka/Kafka-025-%EC%B9%B4%ED%94%84%EC%B9%B4-%EB%B2%84%EB%A1%9C%EC%9A%B0/
github.com/linkedin/Burrow
 
 카프카 버로우 - 컨슈머 랙 평가(Evaluation) 
 버루에서는 임계치가 아닌 슬라이딩 윈도우(sliding window) 계산을 통해 문제가 생긴 파티션과 컨슈머의 상태를 표현한다.
 이렇게 버로우에서 컨슈머 랙의 상태를 표현하는 것을 컨슈머 랙평가 라고 부른다. 컨슈머 랙과 파티션의 오프셋을 슬라이딩 윈도우로 계산하면 
 상태가 정해진다. 결과적으로 파티션의 상태를 OK, STALLED, STOPPED로 표현하고 컨슈머의 상태를 OK, WARNING, ERROR로 표현한다. 
 
 컨슈머 랙 모니터링 아키텍쳐
 버로우를 통해 컨슈머 랙을 모니터링할 때는 이미 지나간 컨슈머 랙을 개별적으로 모니터링하기 위해서 별개의 저장소와 대시보드를 사용하는 것이 효과적 
 이다. 컨슈머 랙 모니터링을 위해 사용할 수 있는 저장소와 대시보드는 다양하지만 빠르게, 무료로 설치할 수 있는 아키텍쳐를 제안한다.
 버로우 : github.com/linkedin/Burrow
 텔레그래프 : github.com/influxdata/telegraf
 엘라스틱서치 : www.elastic.co/kr
 그라파나 : grafana.com
 설치 방법 : blog.voidmainvoid.net/297
 
============================================================================================
멱등성 프로듀서 

 전달 신뢰성
 멱등성이란 여러 번 연산을 수행하더라도 동일한 결과를 나타내는 것을 뜻한다. 이러한 의미에서 멱등성 프로듀서는 동일한 데이터를 여러 번 
 전송하더라도 카프카 클러스터에 단 한 번만 저장됨을 의미한다. 기본 프로듀서의 동작 방식은 적어도 한번 전달(al least once delivery)을 지원한다.
 적어도 한번 전달이란 프로듀서가 클러스터에 데이터를 전송하여 저장 할떄 적어도 한번 이상 데이터를 적재할 수 있고 데이터가 유실되지 않음을 뜻한다.
 다만, 두 번 이상 적재할 가능성이 있으므로 데이터의 중복이 발생할 수 있다.
 - At least once : 적어도 한번 이상 전달
 - At most once : 최대 한번 전달
 - Exactly once : 정확히 한번 전달 
 
 프로듀서가 보내는 데이터의 중복 적재를 막기 위해 0.11.0 이후 버전부터는 프로듀서에서 enable.idempotence 옵션을 사용하여 정확히 
 한번 전달(exctly once delivery)을 지원 한다.
 enable.idempotence 옵션의 기본값은 false 이며 정확히 한번 전달을 위해서는 true로 옵션값을 설정해서 멱등성 프로듀서로 동작하도록 만들면 된다.
 카프카 3.0.0. 부터는 enable.idempotence 옵션값의 기본값은 true(acks=all)로 변경됨
 (acks=all) <=== 리더파티션과 팔로우파티션 모두 데이터 적재되면 안전하게 저장되었다고 리턴 

 멱등성 프로듀서는 기본 프로듀서와 달리 데이터를 브로커로 전달할 때 프로듀셔 PID(Producer unique ID)와 시퀀스 넘버(sequence number)를 함께 전달.
 그러면 브로커는 프로듀서의 PID와 시퀀스 넘버를 확인하여 동일한 메시지의 적재 요청이 오더라도 단 한 번만 데이터를 적재함으로써 프로듀서의 데이터
 는 정확히 한번 브로커에 적재되도록 동작한다.
 - PID : 프로듀서의 고유한 ID
 - SID : 레코드의 전달 번호 ID

 멱등성 프로듀서의 한계
 멱등성 프로듀서는 동일한 세션에서만 정확히 한번 전달을 보장한다. 여기서 말하는 동일 한 세션이란 PID의 생명주기를 뜻한다.
 만약 멱등성 프로듀서로 동작하는 프로듀서 애플리케이션에 이슈가 발생하여 종료되고 애플리케이션을 재시작하면 PID가 달라진다.
 동일한 데이터를 보내더라도 PID가 달라지면 브로커 입장에서 다른 프로듀서 애플리케이션이 다른 데이터를 보냈다고 판단하기 때문에 멱등성 
 프로듀서는 장애가 발생하지 않을 경우에만 정확히 한 번 적재하는 것을 보장한다는 점을 고려해야 한다.

 멱등성 프로듀서로 설정할 경우 옵션 
 멱등성 프로듀서를 사용하기 위해 enable.idempotence를 true로 설정하면 정확히 한번 적재하는 로직이 성립되기 위해 프로듀서의 일부 옵션들이 강제로  
 설정된다. 프로듀서의 데이터 재전송 횟수를 정하는 retries는 기본값으로 Integer.MAX_VALUE로 설정되고 acks옵션인 all로 설정된다.
 이렇게 설정되는 이유는 프로듀서가 적어도 한 번 이상 브로커에 데이터를 보냄으로써 브로어케 단 한 번만 데이터가 적재되는 것을 보장하기 위해서인다.
 멱등성 프로듀서는 정확히 한번 브로커에 데이터를 적재하기 위해 정말로 한번 전송하는 것이 아니다. 상황에 따라 프로듀서가 여러 번 전송하되 브로커가
 여러 번 전송된 데이터를 확인하고 중복된 데이터는 적재하지 않는 것이다.

 멱등성 프로듀서 사용시 오류 확인(예를 들어 네트워크 전송 시 시퀀스 2가 빠르고 1이 느리게 브로커에 전달될 경우)
 멱등성 프로듀서의 시퀀스 넘버는 0부터 시작하여 숫자를 1씩 더한 값이 전달된다.
 브로커에서 멱등성 프로듀서가 전송한 데이터의 PID와 시퀀스 넘버를 확인하는 과정에서 시퀀스 넘버가 일정하지 않은 경우에는 
 OutOfOrderSequenceException이 발생할 수 있다. 이 오류는 브로커가 예상한 시퀀스 넘버와 다른 번호의 데이터의 적재 요청이 왔을 때 발생한다.
 OutOfOrderSequenceException이 발생했을 경우에는 시퀀스 넘버의 역전현상이 발생할 수 있기 때문에 순서가 중요한 데이터를 전송하는 프로듀서는 해당
 Exception이 발생했을 경우 대응하는 방안을 고려해야 한다.


 ============================================================================================
트랜잭션 프로듀서의 동작 

카프카에서 트랜잭션은 다수의 파티션에 데이터를 저장할 경우 모든 데이터에 대해 동일한 원자성(atomic)을 만족시키기 위해 사용한다.
원자성을 만족시킨다는 의미는 다수의 데이터를 동일 트랜잭션으로 묶음으로써 전체 데이터를 처리하거나 전체 데이터를 처리하지 않도록 하는것을 의미.
트랜잭션 프로듀서는 사용자가 보낸 데이터를 레코드로 파티션에 저장할 뿐만 아니라 트랜잭션의 시작과 끝을 표현하기 위해 트랜잭션 레코드를 한 개 더 
보낸다.
COMMIT이라는 레코드를 한개 더 보냄.

트랜잭션 컨슈머의 동작 
트랜잭션 컨슈머는 파티션에 저장된 트랜잭션 레코드를 보고 트랜잭션이 완료(commit)되었음을 확인하고 데이터를 가져간다.
트랜잭션 레코드는 실질적인 데이터는 가지고 있지 않으며 트랜잭션이 끝난 상태를 표시하는 정보만 가지고 있따. 

트랜잭선 프로듀서 설정
트랜잭션 프로듀서로 동작하기 위해 transactional.id를 설정해야 한다. 프로듀서별로 고유한 ID값을 사용해야 한다.
init, begin, commit 순서대로 수행되어야 한다. 

configs.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, UUID,randomUUID());
Producer<String, String> producer = new KafkaProducer<>(configs);

producer.initTransactions();
producer.beginTransaction();
producer.send(new ProducerRecord<>(TOPIC, "메시지));
// ... 여러개의 레코드 복수로 send 한다.
producer.commitTransaction();

producer.close();

트랜잭션 컨슈머 설정
크랜잭션 컨슈머는 커밋이 완료된 레코드들만 읽기 위해 isolation.level 옵션을 read_committed로 설정해야 한다.
기본 값은 read_uncommitted로서 트랜잭션 프로듀서가 레코드를 보낸 후 커밋여부와 상관없이 모두 읽는다.
read_committed로 설정한 컨슈머는 커밋이 완료된 레코드들만 읽어 처리한다.

configs.put(ConsumerConfig.ISOLATION_LEVEL_CONFIG, "read_committed");
KafkaConsumer<String, String> consumer = new KafkaConsumer<>(configs);

 ============================================================================================
카프카 스트림즈

프로듀서 ------------------>  카프카 클러스터   <-------------------- 컨슈머
					  |              |
        				  |              |
                                  스트림즈 애플리케이션

카프카 스트림즈 토픽에 적재된 데이터를 실시간으로 변환하여 다른 토픽에 적재하는 라이브러리이다.
스트림즈는 카프카에서 공식적으로 지원하는 라이브러리이다. 매번 파크파 버전이 오를 때마다 스트림즈 자바 라이르버리도 같이 릴리즈 된다.
그렇게 때문에 자바 기반 스트림즈 애플리케이션은 카프카 클러스터와 완벽하게 호환되면서 스트림 처리에 필요한 편리한 기능들을 제공한다.
스트림즈 애플리케이션 또는 카프카 브로커의 장애가 발생하더라도 정확히 한번(exactly once)할 수 있도록 장애 허용 시스템(fault tolerant system)을
가지고 있어서 데이터 처리 안정성이 매우 뛰어나다. 카프카 클러스터를 운영하면서 실시간 스트림 처리를 해야하는 필요성이 있다면 카프카 스트림즈
애플리케이션으로 개발하는 것을 1순위로 고려하는 것이 좋다.

스트림 데이터 처리에 있어 필요한 다양한 기능을 스트림즈DSL로 제공하며 필요하다면 프로세서 API를 사용하여 기능을 확장할 수 있기 때문이다.
컨슈머와 프로듀서를 조합하여 스트림즈가 제공하는 기능과 유사하게 만들 수 있다. 그러나 스트림즈 라이브러리를 통해 제공하는 단 한번의 데이터 처리,
장애 허용 시스템 등의 특징들은 컨슈머와 프로듀서의 조합만으로는 완벽하게 구현하기는 어렵다.
다만, 스트림즈가 제공하지 못하는 기능을 구현할 때는 컨슈머와 프로듀서를 조합하여 구현하면 좋다. 예를 들어, 소스 토픽(사용하는 토픽)과 
싱크 토픽(저장하는 토픽)의 카프카 클러스터가 서로 다른 경우는 스트림즈가 지원하지 않으므로 이때는 컨슈머와 프로듀서 조합으로 직접 클러스터를 
지정하는 방식으로 개발할 수 있다.

 스트림즈 내부 구조
 ----------------       ------------------------------
 |       토픽    |       |  스트림즈 애플리케이션  |
 |    파티션 0 |   -->|        태스크 0               |
 |    파티션 1 |   -->|        태스크 1               |
 |    파티션 2 |   -->|        태스크 2               |
                                 스    레    드

 스트림즈 애플리케이션은 내부적으로 스레드를 1개 이상 생성할 수 있으며, 스레드는 1개 이상의 태스크를 가진다. 
 스트림즈의 태스크(task)는 스트림즈 애플리케이션을 실행하면 생기는 데이터 처리 최소 단위이다.
  만약 3개의 파티션으로 이루어진 토픽을 처리하는 스트림즈 애플리케이션을 실행하면 내부에 3개의 태스크가 생긴다.
 컨슈머의 병렬처리를 위해 컨슈머 그룹으로 이루어진 컨슈머 스레드를 여러개 실행하는 것과 비슷하다고 볼 수 있다.
 카프카 스레드를 늘리는 방법과 동일하게 병렬처리를 위해 파티션과 스트림즈 스레드(또는 프로세스) 개수를 늘림으로써 처리량을 늘릴 수 있다.

 스트림즈 애플리케이션 스케일 아웃 
 실제 운영환경에서는 장애가 발생하더라도 안정적으로 운영할 수 있도록 2개 이상의 서버로 구성하여 스트림즈 애플리케이션을 운영한다.
 이름 통해 일부 스트림즈 애플리케이션 또는 애플리케이션이 실행되는 서버에 장애가 발생하더라도 안전하게 스트림 처리를 할 수 있다.

 토폴로지 
 링형 토폴로지, 트리형 토폴로지, 성형 토폴로지
 카프카 스트림즈의 구조와 사용 방법을 알기 위해서는 우선 토폴로지(topology)와 관련된 개념을 익혀야 한다.
 토폴로지란 2개 이상의 노드들과 선으로 이루어진 집합을 뜻한다. 토폴로지의 종류로는 링형(ring), 트리형(tree), 성형(start)등이 있는데
 스트림즈에서 사용하는 토폴로지는 트리 형태와 유사하다.

 프로세서와 스트림
 카프카 스트림즈에서는 토폴로지를 이루는 노드를 하나의 프로세서(processor)라고 부르고 노드와 노드를 이은 선을 스트림(stream)이라고 부른다.
 스트림은 토픽의 데이터를 뜻하는 데 프로듀서와 컨슈머에서 활용했던 레코드와 동일하다.

 프로세서에는 소스 프로세서, 스트림 프로세서, 싱크 프로세서 3가지가 있다. 소스 프로세서는 데이터를 처리하기 위해 최초로 선언 해야 하는 노드로,
 하나 이상의 토픽에서 데이터를 가져오는 역할을 한다. 스트림 프로세서는 다른 프로세서가 반환한 데이터를 처리하는 역할을 한다. 변환, 분기처리와 
 같은 로직이 데이터 처리의 일종이라고 볼 수 있다. 마지막으로 싱크 프로세서는 데이터를 특정 카프카 토픽으로 저장하는 역할을 하며 스트림즈로 처리된
 데이터의 최종 종착이다.

 스트림즈 DSL과 프로세서 API
 스트림즈DSL(Domain Specific Language)과 프로세서 API 2가지 방법으로 개발 가능하다. 
 스트림즈 DSL은 스트림 프로세싱에 쓰일 만한 다양한 기능등을 자체 API로 만들어 놓았기 때문에 대부분의 변환 로직을 어렵지 않게 개발할 수 있다.
 만약 스트림즈DSL에서 제공하지 않는 일부 기능들의 경우 프로세서 API를 사용하여 구현할 수 있다.
 스트림즈DSL과 프로세서 API가 구현할 수 있는 종류는 다음과 같다
 
스트림즈 DSL로 구현하는 데이터 처리 예시
 - 메시지 값을 기반으로 토픽 분기 처리
 - 지난 10분간 들어온 데이터의 개수 집계 
 
 프로세서 API로 구현하는 데이터 처리 예시
 - 메시지 값의 종류에 따라 토픽을 가변적으로 전송
 - 일정한 시간 간격으로 데이터 처리 

 스트림즈DSL
 스트림즈DSL로 구성된 애플리케이션을 코드로 구현하기 전에 스트림즈 DSL에서 다루는 새로운 개념들에 대해 짚고 넘어가야 한다.
 스트림즈DSL에는 레코드의 흐름을 추상화한 3가지 개념인 KStream, KTable, GlovalKTable이 있다. 이 3가지 개념은 컨슈머, 프로듀서, 프로세서 API에서는
 사용되지 않고 스트림즈 DSL에서만 사용되는 개념이다.

 KStream
 KStream은 레코드의 흐름을 표현한 것으로 메시지 키와 메시지 값으로 구성되어 있다. KStream 으로 데이터를 조회하면 토픽에 존재하는(또는 KStream
 에 존재하는) 모든 레코드가 출력된다. KStream 컨슈머로 토픽을 구독하는 것과 동일한 선상에서 사용하는 것이라고 볼 수 있다. 

 KTable
 KTable KStream과 다르게 메시지 키를 기준으로 묶어서 사용한다. KStream은 토픽의 모든 레코드를 조회할 수 있지만 KTable은 유니크한 메시지 키를 
 기준으로 가장 최신 레코드를 사용한다. 그러므로 KTable로 데이터를 조회하면 메시지 키를 기준으로 가장 최신에 추가된 레코드의 데이터가 출력된다.
 새로 데이터를 적재할 때 동일한 메시지 키가 있을 경우 데이터가 업데이트 되었다고 볼 수 있다. 왜냐하면 메시지 키의 가장 최신 레코드가 추가되었기
 때문이다.

 코파티셔닝
 KStream과 KTable 데이터를 조인한다고 가정하자. KStream과 KTable을 조인하려면 반드시 코파티셔닝(co-partitioning)되어 있어야 한다. 
 코파티셔닝이란 조인을 하는 2개 데이터의 파티션 개수가 동일하고 파티셔닝 전략(partitioning strategy)을 동일하게 맞추는 작업이다.
 파티션 개수가 동일하고 파티셔닝 전략이 같은 경우에는 동일한 메시지 키를 가진 데이터가 동일한 태스크에 들어가는 것을 보장한다.
 이를 통해 각 태스크는 KStream의 레코드와 KTable의 메시지 키가 동일할 경우 조인을 수행할 수 있다.
 
 코파티셔닝되지 않은 2개 토픽의 이슈 
 문제는 조인을 수행하려는 토픽들이 코파티셔닝되어 있음을 보장할 수 없다는 것이다. KStream과 KTable로 사용하는 2개의 토픽이 파티션 개수가 다를
 수도 있고 파티션 전략이 다를 수 있다. 이런경우에는 조인을 수행할 수 없다. 코파티셔닝이 되지 않은 2개의 토픽을 조인하는 로직이 담긴 스트림즈
 애플리케이션을 실행하면 Topology Exception이 발생한다.
 1. 파티션개수가 동일해야한다.
 2. 파티셔닝 전략이 같아야 한다. 

 GlobalKTable
 코파티셔닝 되지 않은 KStream과 KTable을 조인해서 사용하고 싶다면 KTable을 GlobalKTable로 선언하여 사용하면 된다.
 GlobalKTable은 코파티셔닝되지 않은 KStream과 데이터 조인을 할 수 있따. 왜냐하면 KTable과 다르게 GlobalKTable로 정의된 데이터는 
 스트림즈 애플리케이션의 모든 태스크에 동일하게 공유되어 사용되기 때문이다.

 스트림즈DSL 중요 옵션(필수 옵션)
 - bootstrap.servers : 프로듀서가 데이터를 전송할 대상 카프카 클러스터에 속한 브로어의 호스트 이름:포트를 1개 이상 작성한다.         
                             2개 이상 브로커 정보를 입력하여 일부 브로커에 이슈가 발생하더라도 접속하는 데에 이슈가 없도록 설정 가능하다.
 - application.id : 스트림즈 애플리케이션을 구분하기 위한 고유한 아이디를 설정한다. 다른 로직을 가진 스트림즈 애플리케이션들은 서로 다른
                        application.id값을 가져야 한다.

 스트림즈 DSL 중요 옵션 (선택 옵션)
 - default.key.serde : 레코드의 메시지 키를 직렬화, 역직렬화하는 클래스를 지정한다. 
                            기본값은 바이트 직렬화, 역직렬화 클래스인 Serdes.ByteArray(),getClass().getName()이다.
 - default.value.serde : 레코드의 메시지 값을 직렬화, 역직렬화하는 클래스를 지정한다.
                               기본값은 바이트 직렬화, 역직렬화 클래스인 Serdes.ByteArray(),getClass().getName()이다.
 - num.stream.threads : 스트림 프로세싱 실행 시 실행될 스레드 개수를 지정한다. 기본값은 1 이다.
 - state.dir : 상태기반 데이터 처리를 할 때 데이터를 저장할 디렉토리를 지정한다. 기본값은 /tmp/kafka-streams이다.

 스트림즈DSL - window processing (취합 연산)
 스트림 데이터를 분석할 때 가장 많이 활용하는 프로세싱 중 하나는 윈도우 연산이다. 윈도우 연산은 특정 시간에 대응하여 취합 연산을 처리할 때 
 활용한다. 카프카 스트림즈에서 제공하는 윈도우 프로세싱 4가지를 지원한다. 모든 프로세싱은 메시지 키를 기준으로 취합된다.
 그러므로 해당 토픽에 동일한 파티션에는 동일한 메시지 키가 있는 레코드가 존재해야지만 정확한 취합이 가능하다. 만약 커스텀 파티셔너를 사용하여
 동일 메시지 키가 동일한 파티션에 저장되는 것을 보장하지 못하거나 메시지 키를 넣지 않으면 관련 연산이 불가능하다.
 카프카 스트림즈에서 제공하는 윈도우 연산 종류는 다음과 같다.
 - 텀블링 윈도우
 - 호핑 윈동우 
 - 슬라이딩 윈도우
 - 세션 윈도우 

 - 텀블링 윈도우
 텀블링 윈도우는 서로 겹치지 않은 윈도우를 특정 간격으로 지속적으로 처리할 때 사용한다. 윈도우 최대사이즈에 도달하면 해당 시점에 데이터를
 취합하여 결과를 도출한다. 텀블링 윈도우는 단위 시간당 데이터가 필요한 경우 사용할 수 있다. 예를 들어 매 5분간 접속한 고객의 수를 측정하여 방문자
 추이를 실시간 취합하는 경우 텀블링 윈도우를 사용할 수 있다. 
 지난 1시간동안 접속한 고객의 수등 추출할 경우 

 - 호핑 윈도우
 호핑 윈도우는 일정 시간 간격으로 겹치는 윈도우가 존재하는 윈도우 연산을 처리할 경우 사용한다.
 호핑 윈도우는 윈도우 사이즈와 윈도우 간격 2가지 변수를 가진다. 윈도우 사이즈는 연산을 수행할 최대 윈도우 사이즈를 뜻하고 윈도우 간격은 서로 다른
 윈도우 간 간격을 뜻한다. 텀블링 윈도우와 다르게 동일한 키의 데이터는 서로 다른 윈도우에서 여러번 연산될 수 있다.

 - 슬라이딩 윈도우
 슬라이딩 윈도우는 호핑 윈도우와 유사하지만 데이터의 정확한 시간을 바탕으로 윈도우 사이즈에 포함되는 데이터를 모두 연산에 포함시키는 특징이 있다.

 - 세션 윈도우
 세션 윈도우는 동일 메시지 키의 데이터를 한 세션에 묶어 연산할 때 사용한다. 세션의 최대 만료시간에 따라 윈도우 사이즈가 달라진다.
 세션 만료 시간이 지나게 되면 세션 윈도우가 종료되고 해당 윈도우의 모든 데이터를 취합하여 연산한다.
 그렇게 때문에 세션 윈도우의 윈도우 사이즈는 가변적이다. 

 윈도우 연산시 주의해야할 사항 
 카프카 스트림즈는 커밋(기본 값 30초)을 수행할 때 윈도우 사이즈가 종료되지 않아도 중간 정산 데이터를 출력한다.
 커밋 시점마다 윈도우의 연산 데이터를 출력하기 때문에 동일 윈도우 사이즈(시간)의 데이터는 2개 이상 출력 될 수 있다.

 최종적으로 각 윈도우에 맞는 데이터를 출력하고 싶다면 Windowed를 기준으로 동일 윈도우 시간 데이터는 겹처쓰기(upsert)하는 방식으로 처리하는
 것이 좋다. 예를 들어 0~5초의 A  데이터가 포함된 윈도우 취합 데이터가 들어오면 해당 데이터를 유니크 키로 설정하고 새로 들어온 데이터를 겹처쓰는 
 것이다. 위경우에는 최초에 0~5초 A데이터가 2개 취합된 데이터가 처음 저장되고 , 추후에 6초에 출력된 3개 취합된 데이터가 최종 저장된다. 
 결과적으로 A가 0~5초에 3개 count된것을 확인 할 수 있다.

 스트림즈DSL - Queryable store
 카프카 스트림즈에서 KTable은 카프카 토픽의 데이터를 로컬의 rocksDB에 Materialized View로 만들어 두고 사용하기 때문에 레코드의 메시지 키,
 메시지 값을 기반으로 keyValueStroe로 사용할 수 있다. 
 특정 토픽을 KTable로 사용하고 ReadOnlyKeyValueStore로 뷰를 가져오면 메시지 키를 기반으로 토픽 데이터를 조회할 수 있게 된다.
 마치 카프카를 사용하여 로컬 캐시를 구현한것과 유사하다고 볼 수 있다.
 
 프로세서API
 프로세서API는 스트림즈DSL보다 투박한 코드를 가지지만 토폴로지를 기준으로 데이터를 처리한다는 관점에서는 동일한 역할을 한다.
 스트림즈DSL은 데이터 처리, 분기, 조인을 위한 다양한 메서드들을 제공하지만 추가적인 상세 로직의 구현이 필요하다면 프로세서 API를 활용할 수 있다.
 프로세서 API에서는 스트림즈DSL에서 사용했던 KStream, KTable, GlobalKTable 개념이 없다는 점을 주의해야 한다.
 다만, 스트림즈 DSL과 프로세서API는 함께 구현하여 사용할 때는 활용할 수 있다.
 
 프로세서API를 구현하기 위해서는 Processor 또는 Transformer 인터페이스로 구현한 클래스가 필요하다.
 Processor 인터페이스는 일정 로직이 이루어 진 뒤 다음 프로세서로 데이터가 넘어가지 않을 때 사용한다. 
 반면, Transformer 인터페이스는 일정 로직이 이루어진 뒤 다음 프로세서로 데이터를 넘길 때 사용한다.
 
 카프카 스트림즈 vs스파크 스트리밍 
 - 카프카 스트림즈 : 카프카 토픽을 input으로 하는 경량 프로세싱 애플리케이션 개발
 - 스파크 스트리밍 : 카프카 토픽을 포함한 하둡 생테계(HDFS, hive 등)를 input인 복잡한 프로세싱 개발

 ============================================================================================
카프카 커넥트

주로 토픽에 데이터를 넣거나 뺄때 템플릿 형식으로 파이프라인을 반복적인 생성이 가능하다. 

	         	카프카 
		         토 픽

카프카     소스 커넥터 싱크 커넥터   <=== 스레드 프로세스  (REST API 로 실행시키고 중단시킬수 있음)
커넥트

              소스                싱크 
        애플리케이션    애플리케이션

카프카 커텍트(kafka connect)는 카프카 오픈소스에 포함된 툴 중 하나로 데이터 파이프라인 생성 시 반복 작업을 줄이고 효율적인 전송을 이루기 위한
애플리케이션이다. 커넥트는 특정한 작업 형태를 템플릿으로 만들어놓은 커넥터(connector)를 실행함으로써 반복 작업을 줄일수있다. 


커넥트 내부 구조 

카프카 커넥트
 태스트 #0
 태스트 #1
  커텍터 (한개스레드로 태스크를 관리)

사용자가 커넥트에 커넥터 생성 명령을 내리면 커넥트는 내부에 컨넥터와 태스크를 생성한다. 커넥터는 태스크들을 관리한다.
태스크는 커넥터에 종속되는 개념으로 실질적인 데이터 처리를 한다. 그렇기 때문에 데이터 처리를 정상저긍로 하는 지 확인하기 위해서는 각 태스크의
상태를 확인해야 한다. 
커넥터가 태스크 상태를 확인하는등 VALIDATION 역할을 한다.

소스 커넥터, 싱크 커넥터 크게 2가지가 존재한다.
커넥터는 프로듀서 역할을 하는 '소스 커넥터(source connector)'와 컨슈머 역할을 하는 '싱크 커넥터(sink connector)' 2가지로 나뉜다.
예를 들어, 파일을 주고받는 용도로 파일 소스 커넥터(file source connector)와 파일 싱크 커넥터(file sink connector)가 있다고 가정하자.
파일 소스 커넥터는 파일의 데이터를 카프카 토픽으로 전송하는 프로듀서 역할을 한다. 그리고 파일 싱크 커넥터는 토픽의 데이터를 파일로 저장하는 
컨슈머 역할을 한다. 파일 외에도 일정한 프로토콜을 가진 소스 애플리케이션이나 싱크 애플리케이션이 있다면 커넥터를 통해 카프카로 데이터를 보내거나
카프카에서 데이터를 가져올 수 있다.
MySQL, S3, MongoDB 등과 같은 저장소를 대표적인 싱크 애플리케이션, 소스 애플리케이션이라 볼 수 있다. 즉, MySQL에서 카프카로 데이터를 보낼 때 
그리고 카프카에서 데이터를 MySQL로 저장할 때 JDBC 커넥터를 사용하여 파이프라인을 생성할 수 있다.

커넥트 플러그인
카프카 2.6에 포함된 커넥트를 실행할 경우 클러스터 간 토픽 미러링을 지원하는 미러메이커2 커넥터와 파일 싱크 커넥터, 파일 소스 커넥터를 
기본 플러그인으로 제공한다. 이외에 추가적인 커넥터를 구현하는 클래스를 빌드한 클래스 파일이 포함되어 있다.
커넥터 플러그인을 추가하고 싶다면 직접 커넥터 플러그인을 만들거나 이미 인터넷상에 존재하는 커넥터 플러그인을 가져다 쓸 수도 있다. 

오픈소스 커넥터 
오프손스 커넥터는 직접 커넥터를 만들 필요가 없으며 커넥터 jar파일을 다운로드하여 사용할 수 있다는 장점이 있다. 
HDFS 커넥터, AWS S3 커넥터, JDBC 커넥터, 엘라스틱서치 커넥터 등 100개가 넘는 커넥터들이 이미 공개되어 있다.
필요한 커넥터를 검색하고 아키텍처에 알맞는 커넥터를 찾아서 다운받아 사용하면 된다.
오픈소스 커넥터의 종류는 컨플루언트 허브(https:///confluent.io/hub/)에서 검색할 수 있다. 다만, 오픈 소스라고 모두 무료로 제한없이 사용할 수 있는
것은 아니기 때문에 라이선스를 참고하여 사용 범위를 확인한 이후에 사용해야 한다. 

컨버터, 트랜스폼 (추가적인 플러그인)
사용자가 커넥터를 사용하여 파이프라인을 생성 할 때 컨버터(converter)와 트랜스폼(transform) 기능을 옵션으로 추가할 수 있다.
커넥터를 운영할 때 반드시 필요한 설정을 아니지만 데이터 처리를 더욱 풍부하게 도와주는 역할을 한다.
컨버터는 데이터 처리를 하기 전에 스키마를 변경하도록 도와준다.
JsonConverter, String Converter, ByteArrayConverter를 지원하고 필요하다면 커스텀 컨버터를 작성하여 사용할 수도 있다.
트랜스폼은 데이터 처리 시 각 메시지 단위로 데이터를 간단하게 변환하기 위한 용도로 사용 된다.
예를 들어, JSON 데이터를 커넥터에서 사용할 때 트랜스폼을 사용하면 특정 키를 삭제하거나 추가할 수 있다.
기본 제공 트랜스폼으로 Cast, Drop, ExtractField 등이 있다.

커넥트를 실행하는 방법
커넥트를 실행하는 방법은 크게 두가지가 있다. 단일 모드 커넥트 (standalone mode kafka connect) 이고 두 번째는 분산 모드 커넥트(distributed mode kafka connect)이다. 단일 모드 커넥트는 단일 애플리에키엿느로 샐행된다. 커넥터를 정의하는 파일을 작성하고 해당 파일을 참조하는 단일 모드 커넥트를 실행함으로써 파이프라인을 생성 할 수 있다. 

단일 모드 커넥트 
단일 모드 커넥트는 1개 프로세스만 실행되는 점이 특징인데, 단일 프로세스로 실행되기 때문에 고가용성 구성이 되지 않아서 단일 장애점 failover가 되지 않는다. 그러므로 단일 모드 커넥트 파이프라인은 주로 개발환경이나 중요도가 낮은 파이프라인을 구성할때 활용

분산 모드 커넥트 (2~100개 이상 커넥트 운영)
분산 모드 커넥트는 2대 이상의 서버에서 클러스터 형태로 운영함으로써 단일 모드 커넥트 대비 안전하게 운영할 수 있다는 장점이 있다.
2개 이상의 커넥트가 클러스터로 묶이면 1개의 커넥트가 이슈 발생으로 중단되더라도 남은 1개의 커넥트가 파이프라인을 지속적으로 처리할 수 있다.
분산 모드 커넥트는 데이터 처리량의 변화에도 유연하게 대응할 수 있따. 커넥트가 실행되는 서버 개수를 늘림으로써 무중단으로 스케일 아웃하여 
처리량을 늘릴 수 있기 때문이다. 이러한 장점이 있기 때문에 상용환경에서 커넥트를 운영한다면 분산 모드 커넥트를 2대 이상으로 구성하고 설정하는 
것이 좋다.

커넥트 REST API 인터페이스 
REST API를 사용하면 현재 실행 중인 커넥트의 커넥터 플로그인 종류, 태스크 상태, 커넥터 상태 등을 조회할 수 있다.
커넥트는 8083 포트로 호출할 수 있으며 HTTP 메서드 기반 API를 제공한다.

GET        /            						실행 중인 커넥트 정복 확인
GET       /connectors 						실행 중인 커넥터 이름 확인
POST     /connectors/						새로운 커넥터 생성 요청
GET      /connectors/{커넥터 이름}				실행 중인 커넥터 정보 확인
GET      /connectors/{커넥터 이름}/config		실행 중인 커넥터의 설정값 확인
PUT      /connectors/{커넥터 이름}/config                실행 중인 커넥터 설정값 변경 요청
GET      /connectors/{커넥터 이름}/status			실행 중인 커넥터 상태 확인
POST    /connectors/{커넥터 이름}/restrat		실행 중인 커넥터 재시작 요청


단일 모드 커넥트 설정 
단일 모드 커넥트를 실행하기 위해서는 단일 모드 커넥트를 참조하는 설정 파일인 connect-standalone.properties 파일을 수정해야 한다 .

단일 모드 커넥트의 실행 (파일 단위로 실행하게 된다 connector.class 옵션에 커넥터 클래스를 지정)
단일 모드 커넥트를 실행 시에 파라미터로 커넥트 설정파일과 커넥터 설정파일을 차례로 넣어 실행 한다.


*** 분산 모드 커넥트
분산 모드 커넥트는 단일 모드 커넥트와 다르게 2개 이상의프로세스가 1개의 그룹으로 묶여서 운영된다.
이를 통해 1개의 커넥트 프로세스에 이슈가 발생하여 종료되더라도 살아있는 나머지 1개 커넥트 프로세스가 커넥터를 이어받아서 파이프라인을 
지속적으로 실핼 할 수 있다는 특징이 있다. 이제 분산 모드 커넥트를 묶어서 운옇하기 위해 어떤 설정을 해야 하는지 분산 모드 설정 파일인 
connect-distributed.properties를 살펴보자. 해당 파일은 카프카 바이너리 디렉토리의 config 디렉토리에 있다.

옵션값중 topic에 대해서 다른 그룹이라면 connect- ~ 시리즈의 토픽이름이 겹치지 않도록 한다. 

커스텀 소스 커넥터 

소스 커넥터는 소스 애플리케이션 또는 소스 파일로부터 데이터를 가져와 토픽으로 넣는 역할을 한다.
오픈소스 소스 커넥터를 사용해도 되지만 라이선스 문제나 로직이 원하는 요구사항과 맞지 않아서 직접 개발 해야 하는 경우도 있는데
이때는 카프카 커넥트 라이브러에서 제공하는 SourceConnector와 SorcurceTask 클래스를 사용하여 직접 소스 커넥터를 구현하면 된다. 
직접 구현한 소스 커넥터를 빌드하여 jar파일로 만들고 커넥트를 실행 시 플러그인으로 추가하여 사용할 수 있다. 
토픽에 저장하는 프로듀서 역할과 동일한 역할을 한다. 실질적인 역할을 하는 소스 태스크를 개발 한다. 

커스텀 소스 커넥터 디펜던시 
소스 커넥터를 만들 떄는 connect-api 라이브러리를 추가해야 한다. connect-api 라이브러리에는 커넥터를 개발하기 위한 클래스들이 포함되어 있다. 
    <dependency>
        <groupId>org.apache.kafka</groupId>
        <artifactId>connect-api</artifactId>
        <version>2.5.0</version>
    </dependency>

SourceConnector, SourceTask 총 2가지를 개발하여야 한다.
 SocurceConnector (기본적으로 한개의 스레드에서 실행)
 태스크를 실행하기 전 커넥터 설정파일을 초기 화 하고 어떤 태스크 클래스를 사용할 것인지 정의하는 데에 사용
 실질적인 데이터를 다루는 부분은 들어가지 않는다.

 SourceTask
 소스 애플리케이션 또는 소스 파일로부터 데이터를 가져와서 토픽으로 데이터를 보내는 역할을 수행한다.
 SourceTask만의 특징은 토픽에서 사용하는 오프셋이 아닌 자체적으로 사용하는 오프셋을 사용한다는 점이 있다.
 SourceTask에서 사용하는 오프셋은 소스 애플리케이션 또는 소스 파일을 어디까지 읽었는지 저장하는 역할을 한다.
 이 오프셋을 통해 데이터를 중복해서 토픽으로 보내는 것을 방지할 수 있다. 예를 들어, 파일의 데이터를 한 줄씩 읽어서 토픽으로
 데이터를 보낸다면 토픽으로 데이터를 보낸 줄 번호(line)를 오프셋에 저장할 수 있다.

  커넥터 옵션값 설정시 중요도(Importance) 지정 기준
 커넥터를 개발할 때 옵션값의 중요도를 Importance enum 클래스로 지정할 수 있다. Importance enum 클래스는 HIGH, MEDIUM, LOW 3가지
 종류로 나위어 있다. 결로부터 말하자면 옵션의 Importance를 HIGH, MEDIUM, LOW로 정하는 명확한 기준은 없다. 단지 사용자로 하여금 
 이 옵션이 중요하다는 것을 명시적으로 표시하기 위한 문서로 사용할 뿐이다. 그러므로 커넥터에서 반드시 사용자가 입력한 설정이 필요한 값은
 HIGH,  사용자의 입력값이 없더라도 상관없고 기본값이 있는 옵션을 MEDIUM, 사용자의 입력값이 없어도 되는 옵션을 LOW 정도로 구분하여 지정하면
 된다. importance enum 클래스에 대한 정리 작업은 카프카 내부에서도 논의하고 있다.

 커스텀 싱크 커넥터 (컨슈머 동작과 같음)
 싱크 커넥터는 토픽의 데이터를 타깃 애플리케이션 또는 타깃 파일로 저장하는 역할을 한다.
 카프카 커넥트 라이브러이에서 제공하는 SinkConnector와 SinkTask 클래스를 사용하면 직접 싱크 커넥터를 구현 할 수 있다.
 직접 구현한 싱크 커넥트는 빌드하여 jar로 만들고 커넥트의 플러그인으로 추가하여 사용 할 수 있다. 

 
 분산모드 카프카 커넥트 설정은 config/connect-distributed.properties 파일에서 변경 할 수 있따. 

분산 모드 커넥트 실행 및 플러그인 확인 
./bin/windows/connect-distributed.bat config/connect-distributed.properties

postman에서 get으로 http://localhost:8083/connector-plugins 

FileStreamSinkConnector 테스트 
post 방시긍로 http://localhost:8083/connectors
json 방식으로 값을 넘김

{
	"name" : "file-sink-tesk",
        "config" : {
          "topics" : "test",
          "connector.class": "org.apache.kafka.connect.file.FileStreamSinConnector",
          "tasks.max": 1,
          "file": "/tmp/connect-test.txt"
        }
}


1. config/connect-distributed.properties 설정 변경 
2. ./bin/windows/connect-distributed.bat config/connect-distributed.properties
http://localhost:8083/connectors/file-sink-test/status
 
# 커넥터 플러그인 조회
$ curl -X GET http://localhost:8083/connector-plugins

# FileStreamSinkConnector 생성

$ curl -X POST \
  http://localhost:8083/connectors \
  -H 'Content-Type: application/json' \
  -d '{
    "name": "file-sink-test",
    "config":
    {
	    "topics":"test",
	    "connector.class":"org.apache.kafka.connect.file.FileStreamSinkConnector",
	    "tasks.max":1,
	    "file":"/tmp/connect-test.txt"
    }
  }'

# file-sink-test 커넥터 실행 상태 확인
$ curl http://localhost:8083/connectors/file-sink-test/status

# file-sink-test 커넥터의 태스크 확인
$ curl http://localhost:8083/connectors/file-sink-test/tasks

# file-sink-test 커넥터 특정 태스크 상태 확인
$ curl http://localhost:8083/connectors/file-sink-test/tasks/0/status

# file-sink-test 커넥터 특정 태스크 재시작
$ curl -X POST http://localhost:8083/connectors/file-sink-test/tasks/0/restart

# file-sink-test 커넥터 수정
$ curl -X PUT http://localhost:8083/connectors/file-sink-test/config \
  -H 'Content-Type: application/json' \
  -d '{
	    "topics":"test",
	    "connector.class":"org.apache.kafka.connect.file.FileStreamSinkConnector",
	    "tasks.max":1,
	    "file":"/tmp/connect-test2.txt"
	}'

# file-sink-test 커넥터 중지
$ curl -X PUT http://localhost:8083/connectors/file-sink-test/pause

# file-sink-test 커넥터 시작
$ curl -X PUT http://localhost:8083/connectors/file-sink-test/resume

# file-sink-test 커넥터 재시작
$ curl -X POST http://localhost:8083/connectors/file-sink-test/restart

# file-sink-test 커넥터 삭제
$ curl -X DELETE http://localhost:8083/connectors/file-sink-test

 카프카 커넥트를 운영하기 위한 웹페이지 
 카프카 커넥트는 편리한 rest api 인터페이스를 제공하지만 지속적으로 파이프라인(싱크 커넥터, 소스커넥터)를 운영하는데는 한계가 있어보인다.
 그렇기 때문에 지속적으로 싱크 커넥터, 소스 커넥터를 조회하고 수정 삭제하기 위해서는 웹페이지가 빈드시 필요하다. 커넥트를 운영하기 윈한 웹페이지는
 오픈소스를 사용해도 좋고 직접 만들어서 운영하는 것도 좋다 .
 https://github/com/kakao/kafka-connect-web

커넥트는 프로세스이고 WORKER의 단위


 ============================================================================================
카프카 기본 개념
 - 카프카 : 카프카 클러스터를 포함한 이벤트 스트림 프로세싱 아케틱처.
 - 브로커 : 카프카 클러스터의 서버 중 하나. 데이터를 저장하고 전달하는 역할.
 - 토픽 : 카프카 클러스터에서 데이터를 구분하는 단위. 토픽은 최소 1개 이상의 파티션을 가지고 있음.
 - 파티션 : 토픽에서 데이터를 논리적으로 구분하는 단위.
 - 레코드 : 메시지를 담는 가장 작은 크기.
  - 오프셋 : 프로듀서가 보낸 데이터가 브로커에 저장되었을 때 받는 고유한 번호.
  - 타임스탬프 : 레코드가 프로듀서에서 생성되었을 때 시간(또는 브로커에 적재된 시간).
  - 헤더 : 레코드의 특징을 담는 키/값 저장소.
  - 메시지 키 : 데이터를 구분하는 값. 파티셔너는 메시지 키를 토대로 파티션을 지정.
  - 메시지 값 : 실질적으로 처리하고자 하는 데이터.
 - ISR : 동기화가 완료된 리더, 팔로워 파티션 묶음.

 
 ============================================================================================
카프카 프로듀서 기본 개념
 - 카프카 프로듀서 : 카프카 브로커로 데이터를 전달하는 역할을 하는 애플리케이션.
 - 파티셔너 : 메시지 키를 토대로 파티션을 지정하는 class. 커스텀 클래스를 사용하여 로직 변경 가능.
 - 어큐뮤레이터 : 레코드 전송시 배치로 묶는 역할.
 - acks : 레코드를 카프카 클러스터로 전송시 전달 신뢰성을 지정.
 - min.insync.replicas : acks=all일 경우 최소 적재 보장 파티션 개수 
 - enable.idempotence : 멱등성 프로듀서로 동작하기 위해 설정하는 옵션. (중복이 되지 않게 하기 위한 옵션)
 - transactional.id : 트랜잭션 프로듀서로 동작하기 위해 설정하는 옵션.

 ============================================================================================
카프카 컨슈머 기본 개념
 - 카프카 컨슈머 : 카프카 클러스터에 저장된 레코드를 받아와서 처리하는 애플리케이션
 - 컨슈머 그룹 : 동일한 역할을 하는 컨슈머 들의 묶음.
 - 컨슈머 랙 : 파티션에서 가장 최근의 레코드 오프셋과 컨슈머 오프셋간의 차이.
 - 커밋 : 컨슈머가 레코드 처리가 완료되었을 경우 카프카 클러스터에 마지막으로 읽은 레코드의 오프셋 번호를 저장하는 작업.
 - 리밸런싱 : 컨슈머 그룹에서 컨슈머 개수의 변화 또는 파티션 개수의 변화로 할당이 변경되는 작업.
 - auto.offset.reset : 컨슈머 그룹이 없을 경우 처음 읽을 오프셋의 위치를 지정하는 옵션.
 - isolation.level : 트랜잭션이 완료된 레코드를 읽을 것인지 판단하는 옵션. (한번에 처리하거나, 안하거나)

 ============================================================================================
카프카 스트림즈 기본 개념
 - 카프카 스트림즈 : 상태/비상태 기반 스트림 데이터 처리를 수행하는 애플리케이션.
 - 태스크 : 스트림즈 애플리케이션 내부에서 생성되어 로직을 수행하는 최소 단위.
 - 프로세서 : 데이터를 가져오거나, 처리하거나, 내보내는 노드 (소스, 스트림, 싱크 프로세서)
 - 스트림 : 프로세서로 부터 처리된 데이터를 다른 프로세스로 전달되는 레코드
 - 스트림즈DSL : 추상화되어 스트림 프로세싱에 필요한 메서드들을 정의한 메서드들의 모음.
 - 프로세서API : 스트림즈DSL에서 구현할 수 없는 로직을 구현할 떄 사용하는 API.
 - KStream : 레코드의 흐름. 컨슈머의 poll()과 유사.
 - KTable : 특정 파티션의 메시지 키를 기준으로 가장 최근의 레코드들의 묶음.
 - GlobalktABLE : 모든 파티션의 메시지 키를 기준으로 가장 최근의 레코드들의 묶음.
 - 코파티셔닝 : 동일한 파티션 개수, 동일한 파티셔닝 전략을 통해 레코드가 저장된 서로 다른 2개의 토픽.

 ============================================================================================
카프카 커넥트 기본 개념 
 - 카프카 커넥트 : 카프카 기반 데이터 파이프라인을 반복적으로 생성할 때 사용하는 애플리케이션.
 - 소스 커넥터 : 특정 파일 또는 소스 애플리케이션으로부터 토픽으로 데이터를 보내는 프로듀서.
 - 싱크 커넥터 : 토픽에서 특정 파일 또는 소스 애플리케이션으로 데이터를 보내는 컨슈머.
 - 오픈소스 커넥터 : 소스/싱크 커넥터를 사용할 수 있도록 jar형태로 배포하는 커넥터.
 - 태스크 : 커넥터에서 데이터를 처리하는 최소 로직 단위.
 - 단일 모드 : 디버깅, 테스트용으로 적합한 1프로세스 단위 커넥트.
 - 분산 모드 : 상용환경 운영에 적합한 멀티 프로세스 단위 커넥트 

